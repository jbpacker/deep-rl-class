{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a2c.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uivUVVt7lWxZ",
        "fmA6t3khlbbV",
        "PSbrD6xylfOb",
        "9LHlYvq2mHd0",
        "ke_yTmxkmKE6",
        "yQ_bS0eQnInF",
        "8OQe4Ubmmxro",
        "w9Z25PaaPlO_",
        "gIOVbU19MKVB"
      ],
      "authorship_tag": "ABX9TyNgTvto2YOqyt+gH5/Ey9WF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b9cb9abf687413daa6e2f8908d4938b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f9c1fb35cd740c7b72a75032be9d9ba",
              "IPY_MODEL_08c696b82213471b9bd677f718ea7b8c"
            ],
            "layout": "IPY_MODEL_be05940a70ba4f12b5e25c9ee60addf4"
          }
        },
        "5f9c1fb35cd740c7b72a75032be9d9ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9ada6e0c8c14a06a5398e4d181f3822",
            "placeholder": "​",
            "style": "IPY_MODEL_c2ef78d6eacf4c288ab2ca12b87d73c9",
            "value": "0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "08c696b82213471b9bd677f718ea7b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e99d2f4edfef4610b22d8bf5154a1d5c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ad66eece96d46a5bedc0fd1d54203d0",
            "value": 1
          }
        },
        "be05940a70ba4f12b5e25c9ee60addf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9ada6e0c8c14a06a5398e4d181f3822": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2ef78d6eacf4c288ab2ca12b87d73c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e99d2f4edfef4610b22d8bf5154a1d5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ad66eece96d46a5bedc0fd1d54203d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b99dc90b01c44cdd9b675b17064197f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6ca302d641345dcaa651466db06d88a",
              "IPY_MODEL_5fdbbafcfa7940baba346d02788de255"
            ],
            "layout": "IPY_MODEL_1318eab8f2b84767ab4e95a26f433d2c"
          }
        },
        "a6ca302d641345dcaa651466db06d88a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5a30f428a9341b8b4c2f656bb5b28c4",
            "placeholder": "​",
            "style": "IPY_MODEL_6c2130f0b8a348b59af0949a751a4258",
            "value": "0.661 MB of 0.661 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "5fdbbafcfa7940baba346d02788de255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90c6e4c17ef84ab9b5074c71a29e4fea",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3faf188a6dda410d81de70cb5880c6e1",
            "value": 1
          }
        },
        "1318eab8f2b84767ab4e95a26f433d2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5a30f428a9341b8b4c2f656bb5b28c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c2130f0b8a348b59af0949a751a4258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90c6e4c17ef84ab9b5074c71a29e4fea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3faf188a6dda410d81de70cb5880c6e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbpacker/deep-rl-class/blob/main/unit7/a2c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My a2c implementation\n",
        "\n",
        "* [view results](https://wandb.ai/jefsnacker/a2c_CartPole-v1?workspace=user-jefsnacker)\n",
        "\n",
        "resources:\n",
        "\n",
        "* [a2c walkthrough](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)\n",
        "* [huggingface class](https://huggingface.co/blog/deep-rl-a2c)\n",
        "* [huggingface a2c](https://github.com/huggingface/deep-rl-class/blob/main/unit7/unit7.ipynb)\n",
        "* [a2c commic](https://cdn.discordapp.com/attachments/997489654565712002/1003348192093540462/unknown.png)\n",
        "* [pytorch implementation](https://github.com/pytorch/examples/blob/main/reinforcement_learning/actor_critic.py)\n",
        "* [single step example](https://medium.com/deeplearningmadeeasy/advantage-actor-critic-a2c-implementation-944e98616b) with [code](https://github.com/hermesdt/reinforcement-learning/blob/master/a2c/cartpole_a2c_online.ipynb)\n",
        "* [post on A2C](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)\n"
      ],
      "metadata": {
        "id": "G_hIi2molJKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Everything Ready"
      ],
      "metadata": {
        "id": "Kj6e-_1RlUDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install deps"
      ],
      "metadata": {
        "id": "uivUVVt7lWxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "\n",
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(500, 500))\n",
        "virtual_display.start()\n",
        "\n",
        "!pip install pybullet\n",
        "!pip install gym\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
        "!pip install git+https://github.com/qlan3/gym-games.git\n",
        "!pip install huggingface_hub\n",
        "!pip install wandb\n",
        "!pip install imageio-ffmpeg\n",
        "\n",
        "!pip install pyyaml==6.0 # avoid key error metadata\n",
        "\n",
        "!pip install pyglet # Virtual Screen"
      ],
      "metadata": {
        "id": "CmIF1CNElIoh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ba839bfa-4459-4004-ec86-054c011c45ee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 0s (4,377 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 155680 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 785 kB of archives.\n",
            "After this operation, 2,271 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.11 [785 kB]\n",
            "Fetched 785 kB in 0s (7,339 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 158035 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.11_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.11) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.11) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 91.7 MB 32 kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.7.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-1.6.0-py3-none-any.whl (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 19.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.21.6)\n",
            "Collecting gym==0.21\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 66.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.12.1+cu113)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.5)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.8.0)\n",
            "Collecting protobuf~=3.19.0\n",
            "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 61.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.6.0.66)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Collecting ale-py==0.7.4\n",
            "  Downloading ale_py-0.7.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 67.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.9.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py==0.7.4->stable-baselines3[extra]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py==0.7.4->stable-baselines3[extra]) (4.1.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.47.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2022.1)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616826 sha256=18afcd09df8ee5c239a0cda43244b2b7b6f456d71186a17705e397009c802274\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=17fdb6c4a22ac81ce35c0259b39af11d1ebfaf73bc2eddb50866fb7ca31b036a\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: protobuf, gym, AutoROM.accept-rom-license, autorom, stable-baselines3, ale-py\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.4 autorom-0.4.2 gym-0.21.0 protobuf-3.19.4 stable-baselines3-1.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
            "  Cloning https://github.com/ntasfi/PyGame-Learning-Environment.git to /tmp/pip-req-build-wc7_jm6q\n",
            "  Running command git clone -q https://github.com/ntasfi/PyGame-Learning-Environment.git /tmp/pip-req-build-wc7_jm6q\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ple==0.0.1) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from ple==0.0.1) (7.1.2)\n",
            "Building wheels for collected packages: ple\n",
            "  Building wheel for ple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ple: filename=ple-0.0.1-py3-none-any.whl size=50791 sha256=7a18acf638d68d0da11947f360bb16eb024d5fb29b4b8e0fef43298da5692b05\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-46juc_gr/wheels/cd/51/18/46ce3a7c7b4a75d9ba91594b40e028f98b2001414f6c1da798\n",
            "Successfully built ple\n",
            "Installing collected packages: ple\n",
            "Successfully installed ple-0.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/qlan3/gym-games.git\n",
            "  Cloning https://github.com/qlan3/gym-games.git to /tmp/pip-req-build-6azci77c\n",
            "  Running command git clone -q https://github.com/qlan3/gym-games.git /tmp/pip-req-build-6azci77c\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from gym-games==1.0.4) (1.21.6)\n",
            "Collecting MinAtar>=1.0.4\n",
            "  Downloading MinAtar-1.0.10-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: gym>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from gym-games==1.0.4) (0.21.0)\n",
            "Requirement already satisfied: setuptools>=41.0.1 in /usr/local/lib/python3.7/dist-packages (from gym-games==1.0.4) (57.4.0)\n",
            "Collecting pygame>=1.9.6\n",
            "  Downloading pygame-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ple>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from gym-games==1.0.4) (0.0.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.13.0->gym-games==1.0.4) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym>=0.13.0->gym-games==1.0.4) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym>=0.13.0->gym-games==1.0.4) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym>=0.13.0->gym-games==1.0.4) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (2022.1)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.7.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.15.0)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (0.11.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (0.11.0)\n",
            "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (3.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from ple>=0.0.1->gym-games==1.0.4) (7.1.2)\n",
            "Building wheels for collected packages: gym-games\n",
            "  Building wheel for gym-games (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-games: filename=gym_games-1.0.4-py3-none-any.whl size=14632 sha256=be3b4f8412cb9ae80dce0cf5ae946a6b0178b3d066753cf4ba3d80cd70e73022\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jljyagml/wheels/4d/32/86/19e03a5c068f41f6f2dd6f4197d893f97a27ae182a66bf598e\n",
            "Successfully built gym-games\n",
            "Installing collected packages: pygame, MinAtar, gym-games\n",
            "Successfully installed MinAtar-1.0.10 gym-games-1.0.4 pygame-2.1.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 10.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.12.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 67.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface_hub) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (1.24.3)\n",
            "Installing collected packages: pyyaml, huggingface-hub\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.1-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 26.3 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 66.9 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 72.3 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.19.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 65.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 73.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 80.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 74.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=46b0b2c9d536b5f12e0293cb27b4db6b9cad5eed3205bd5b31e275c5aeef5270\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting imageio-ffmpeg\n",
            "  Downloading imageio_ffmpeg-0.4.7-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.9 MB 75.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: imageio-ffmpeg\n",
            "Successfully installed imageio-ffmpeg-0.4.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml==6.0 in /usr/local/lib/python3.7/dist-packages (6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "fmA6t3khlbbV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ebJznj-NlFXd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import wandb\n",
        "\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import gym_pygame\n",
        "\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select training device"
      ],
      "metadata": {
        "id": "PSbrD6xylfOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "NLNhNj6Jlfsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abecd4e7-270b-453e-fb89-a811aa96aa50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Networks"
      ],
      "metadata": {
        "id": "ML-H33-ilo9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Critic"
      ],
      "metadata": {
        "id": "9LHlYvq2mHd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, num_obs):\n",
        "        \"\"\"\n",
        "        Takes the state as input and outputs Q(s), which is\n",
        "        a vector of Q values for all possible actions\n",
        "        \"\"\"\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        \n",
        "        self.num_obs = num_obs\n",
        "\n",
        "        self.l1 = nn.Linear(num_obs, 128)\n",
        "        self.l2 = nn.Linear(128, 256)\n",
        "        self.l3 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.l2(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        return self.l3(x)\n",
        "    \n",
        "    ## Used if model output is Q\n",
        "    # def get_all_q(self, state):\n",
        "    #     state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "    #     qs = self.forward(state)\n",
        "    #     return qs\n",
        "\n",
        "    # def get_q(self, state, action):\n",
        "    #     return self.get_all_q(state)[:,action]\n",
        "\n",
        "## Debugging\n",
        "# env = gym.make(\"CartPole-v1\")\n",
        "# c = CriticNetwork(env.observation_space.shape[0], env.action_space.n)\n",
        "# print(c)\n",
        "# s = env.reset()\n",
        "\n",
        "# print(c.get_all_q(s))\n",
        "# print(c.get_q(s, 1))"
      ],
      "metadata": {
        "id": "W1i6uKwAlun1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor"
      ],
      "metadata": {
        "id": "ke_yTmxkmKE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, num_obs, num_act):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        \n",
        "        self.num_obs = num_obs\n",
        "        self.num_act = num_act\n",
        "\n",
        "        self.l1 = nn.Linear(num_obs, 128)\n",
        "        self.l2 = nn.Linear(128, 256)\n",
        "        self.l3 = nn.Linear(256, num_act)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.l2(x)\n",
        "        x = F.relu(x)\n",
        "        action_scores = self.l3(x)\n",
        "        action_probs = F.softmax(action_scores, dim=1)\n",
        "\n",
        "        return action_probs\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Given a state, take action\n",
        "        \"\"\"\n",
        "        probs = self.forward(state)\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action, m.log_prob(action)"
      ],
      "metadata": {
        "id": "E-b92LwRlr0y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ktuxcFrKmvzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### utils"
      ],
      "metadata": {
        "id": "yQ_bS0eQnInF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_networks(env):\n",
        "    num_obs = env.observation_space.shape[0]\n",
        "    num_act = env.action_space.n\n",
        "\n",
        "    actor = ActorNetwork(num_obs, num_act)\n",
        "    critic = CriticNetwork(num_obs)\n",
        "\n",
        "    return actor, critic"
      ],
      "metadata": {
        "id": "FlODeOjmnfoQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def record_video(env, policy, out_directory, fps=30):\n",
        "    images = []  \n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "    while not done:\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        with torch.no_grad():\n",
        "            action, _ = policy.act(torch.from_numpy(state).float().unsqueeze(0))\n",
        "            state, reward, done, info = env.step(action.item()) # We directly put next_state = state for recording logic\n",
        "            img = env.render(mode='rgb_array')\n",
        "            images.append(img)\n",
        "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
        "    wandb.log({\"videos\": wandb.Video(out_directory, fps=fps)})\n",
        "\n",
        "# env_id = \"CartPole-v1\"\n",
        "# env = gym.make(env_id)\n",
        "# policy = PolicyNetwork(num_obs, num_act)\n",
        "# record_video(env, policy, \"/home/out.gif\", fps=30)"
      ],
      "metadata": {
        "id": "CIRZITDCnILn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "8OQe4Ubmmxro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env_id):\n",
        "    if log: \n",
        "        name = \"a2c_\" + env_id\n",
        "        wandb.init(project=name)\n",
        "\n",
        "    env = gym.make(env_id)\n",
        "    reward = 0\n",
        "    done = False\n",
        "\n",
        "    actor, critic = make_networks(env)\n",
        "\n",
        "    if log:\n",
        "        wandb.watch((actor, critic), log_freq=1)\n",
        "\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=a_lr)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=c_lr)\n",
        "\n",
        "\n",
        "    # (next_state)\n",
        "    #      o\n",
        "    next_state = env.reset()\n",
        "\n",
        "    episode_steps = 0\n",
        "    episode_reward = 0\n",
        "    num_episodes = 1\n",
        "\n",
        "    for step in range(1, n_steps):\n",
        "        #      (state)\n",
        "        #  (-->)  o\n",
        "        state = next_state\n",
        "\n",
        "        #      (state)  r,a  (next_state)\n",
        "        #  (-->)  o ------------> o\n",
        "        action, log_prob = actor.act(state)\n",
        "        next_state, reward, done, info = env.step(action.item())\n",
        "        if done:\n",
        "            advantage = reward - critic(state)\n",
        "        else:\n",
        "            advantage = reward + gamma*critic(next_state) - critic(state)\n",
        "\n",
        "        episode_steps += 1\n",
        "        episode_reward += reward\n",
        "\n",
        "        ## update critic\n",
        "        critic_loss = advantage.pow(2).mean()\n",
        "        critic_loss.backward()\n",
        "\n",
        "        critic_optimizer.step()\n",
        "        critic_optimizer.zero_grad()\n",
        "\n",
        "        ## update actor\n",
        "        # detach advantage to update the 2nd network\n",
        "        actor_loss = -log_prob * advantage.detach()\n",
        "        actor_loss.backward()\n",
        "\n",
        "        actor_optimizer.step()\n",
        "        actor_optimizer.zero_grad()\n",
        "\n",
        "        ## If done next step them reset env\n",
        "        if done or episode_steps > max_episode_steps:\n",
        "\n",
        "            if log:\n",
        "                wandb.log({\"episode_steps\": episode_steps,\n",
        "                           \"episode_reward\": episode_reward,\n",
        "                           \"num_episodes\": num_episodes})\n",
        "\n",
        "            # (next_state, next_done)\n",
        "            #           o\n",
        "            next_state = env.reset()\n",
        "            episode_steps = 0\n",
        "            episode_reward = 0\n",
        "            num_episodes += 1\n",
        "\n",
        "\n",
        "        if log and step % log_rate == 0:\n",
        "            wandb.log({\"step\": step, \n",
        "                       \"actor_loss\": actor_loss,\n",
        "                       \"critic_loss\": critic_loss,\n",
        "                       \"advantage\": advantage})\n",
        "            if record_vids and num_episodes % num_episodes_to_vid == 0:\n",
        "                record_video(env, actor, \"/out.mp4\")"
      ],
      "metadata": {
        "id": "x3e9FyXpmxRW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### episodic implementation\n",
        "[example](https://github.com/hermesdt/reinforcement-learning/blob/master/a2c/cartpole_a2c_episodic.ipynb)"
      ],
      "metadata": {
        "id": "y39bWjOGMppk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class CombinedActorCriticNetwork(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        num_obs = env.observation_space.shape[0]\n",
        "        num_actions = env.action_space.n\n",
        "        self.actor = nn.Sequential(\n",
        "          layer_init(nn.Linear(num_obs, 64)),\n",
        "          nn.ReLU(),\n",
        "        #   layer_init(nn.Linear(64, 64)),\n",
        "        #   nn.ReLU(),\n",
        "          layer_init(nn.Linear(64, num_actions), std=1.0)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "          layer_init(nn.Linear(num_obs, 64)),\n",
        "          nn.ReLU(),\n",
        "        #   layer_init(nn.Linear(64, 64)),\n",
        "        #   nn.ReLU(),\n",
        "          layer_init(nn.Linear(64, 1), 0.01)\n",
        "        )\n",
        "\n",
        "    def value(self, x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def act(self, x):\n",
        "        logits = self.actor(x)\n",
        "        probs = Categorical(logits=logits)\n",
        "        action = probs.sample()\n",
        "        return action, probs.log_prob(action)\n",
        "\n",
        "\n",
        "def train_epi2(env_id, log=False):\n",
        "    training_cycles = 5000\n",
        "    num_episodes_to_vid = 100\n",
        "    lr = 3e-2\n",
        "    gamma = 0.99\n",
        "    eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "    if log:\n",
        "        name = \"a2c_\" + env_id\n",
        "        wandb.init(project=name)\n",
        "\n",
        "    env = gym.make(env_id)\n",
        "    agent = CombinedActorCriticNetwork(env)#.to(device)\n",
        "\n",
        "    if log:\n",
        "        wandb.watch(agent, log_freq=1)\n",
        "\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=lr, eps=1e-5)\n",
        "\n",
        "    for i in range(training_cycles):\n",
        "        logprobs = []\n",
        "        rewards = []\n",
        "        values = []\n",
        "\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_steps = 0\n",
        "        for s in range(10000):\n",
        "            action, logprob = agent.act(torch.Tensor(state).float())\n",
        "            value = agent.value(torch.Tensor(state).float())\n",
        "\n",
        "\n",
        "            state, reward, done, info = env.step(action.item())\n",
        "\n",
        "            logprobs.append(logprob)\n",
        "            values.append(value)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            episode_reward += reward\n",
        "            episode_steps = s\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        R = 0\n",
        "        returns = []\n",
        "        for r in reversed(rewards):\n",
        "            R = r + gamma*R\n",
        "            returns.insert(0, R)\n",
        "\n",
        "        # convert everybody to tensors!\n",
        "        returns = torch.Tensor(returns).unsqueeze(axis=1)\n",
        "        avg_returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "        ## NOTE: the for loop is here to preserve the grad_fn from each of the \n",
        "        # values/logprobs in the list. Without these backprop doesn't work!\n",
        "        policy_losses = [] # list to save actor (policy) loss\n",
        "        value_losses = [] # list to save critic (value) loss\n",
        "        for logprob, value, r in zip(logprobs, values, avg_returns):\n",
        "            advantage = r - value.item()\n",
        "            # calculate actor (policy) loss \n",
        "            policy_losses.append(-logprob * advantage)\n",
        "\n",
        "            # calculate critic (value) loss using L1 smooth loss\n",
        "            value_losses.append(F.smooth_l1_loss(value, torch.tensor([r])))\n",
        "\n",
        "        # reset gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
        "\n",
        "        # perform backprop\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if log:\n",
        "            wandb.log({\n",
        "                \"episode_steps\": episode_steps,\n",
        "                \"episode_reward\": episode_reward,\n",
        "                \"num_episodes\": i,\n",
        "                \"loss\": loss,\n",
        "            })\n",
        "            if i % num_episodes_to_vid == 0:\n",
        "                    record_video(env, agent, \"/content/out.mp4\")\n",
        "\n",
        "# env_id = \"CartPole-v1\"\n",
        "env_id = \"Pixelcopter-PLE-v0\"\n",
        "\n",
        "train_epi2(env_id, log=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246,
          "referenced_widgets": [
            "0b9cb9abf687413daa6e2f8908d4938b",
            "5f9c1fb35cd740c7b72a75032be9d9ba",
            "08c696b82213471b9bd677f718ea7b8c",
            "be05940a70ba4f12b5e25c9ee60addf4",
            "f9ada6e0c8c14a06a5398e4d181f3822",
            "c2ef78d6eacf4c288ab2ca12b87d73c9",
            "e99d2f4edfef4610b22d8bf5154a1d5c",
            "3ad66eece96d46a5bedc0fd1d54203d0"
          ]
        },
        "id": "Y0sNcDVZuO86",
        "outputId": "2fada8d7-ae50-429d-91d6-215322ad9f64"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:20jkq3o5) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b9cb9abf687413daa6e2f8908d4938b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">laced-silence-47</strong>: <a href=\"https://wandb.ai/jefsnacker/a2c_CartPole-v1/runs/20jkq3o5\" target=\"_blank\">https://wandb.ai/jefsnacker/a2c_CartPole-v1/runs/20jkq3o5</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220815_100708-20jkq3o5/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:20jkq3o5). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220815_100935-155w70rt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/jefsnacker/a2c_Pixelcopter-PLE-v0/runs/155w70rt\" target=\"_blank\">volcanic-jazz-8</a></strong> to <a href=\"https://wandb.ai/jefsnacker/a2c_Pixelcopter-PLE-v0\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.1.2 (SDL 2.0.16, Python 3.7.13)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "couldn't import doomish\n",
            "Couldn't import doom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_episodic(env_id):\n",
        "    if log: \n",
        "        name = \"a2c_\" + env_id\n",
        "        wandb.init(project=name)\n",
        "\n",
        "    eps = np.finfo(np.float32).eps.item()\n",
        "    env = gym.make(env_id)\n",
        "    reward = 0\n",
        "    done = False\n",
        "\n",
        "    actor, critic = make_networks(env)\n",
        "\n",
        "    if log:\n",
        "        wandb.watch((actor, critic), log_freq=1)\n",
        "\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=a_lr)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=c_lr)\n",
        "\n",
        "    num_episodes = 1\n",
        "    episode_steps = 1\n",
        "    episode_reward = 0\n",
        "    next_state = env.reset()\n",
        "\n",
        "    # each episode collects 1 episode or N steps and trains\n",
        "    for epoch in range(n_epochs):\n",
        "        # (next_state)\n",
        "        #      o\n",
        "        rewards = []\n",
        "        log_probs = []\n",
        "        values = []\n",
        "\n",
        "        for step in range(0, max_episode_steps):\n",
        "            #      (state)\n",
        "            #  (-->)  o\n",
        "            state = next_state\n",
        "\n",
        "            #      (state)  r,a  (next_state)\n",
        "            #  (-->)  o ------------> o\n",
        "            action, log_prob = actor.act(torch.from_numpy(state).float().unsqueeze(0))\n",
        "            next_state, reward, done, info = env.step(action.detach().item())\n",
        "\n",
        "            values.append(critic(torch.from_numpy(state).float().unsqueeze(0)))\n",
        "            rewards.append(reward)\n",
        "            log_probs.append(log_prob)\n",
        "\n",
        "            episode_reward += reward\n",
        "            episode_steps += 1\n",
        "\n",
        "            # print(\"[{}] a: {} r: {} d: {}\".format(step, advantage, reward, done))\n",
        "\n",
        "            if done or episode_steps > max_episode_steps:\n",
        "                if log:\n",
        "                    wandb.log({\n",
        "                        \"episode_steps\": episode_steps,\n",
        "                        \"episode_reward\": episode_reward,\n",
        "                        \"num_epochs\": epoch,\n",
        "                        \"num_episodes\": num_episodes,\n",
        "                    })\n",
        "\n",
        "                num_episodes += 1\n",
        "                episode_reward = 0\n",
        "                episode_steps = 1\n",
        "                next_state = env.reset()\n",
        "                break\n",
        "\n",
        "        R = 0\n",
        "        returns = []\n",
        "        for r in reversed(rewards):\n",
        "            R = r + gamma*R\n",
        "            returns.insert(0, R)\n",
        "\n",
        "        returns = torch.Tensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "        # for i in range(len(rewards)):\n",
        "        #     print(\"[{}] rew: {} val: {} lp: {} R: {}\".format(i, rewards[i], values[i], log_probs[i], returns[i]))\n",
        "\n",
        "        # print(\"values\")\n",
        "        # print(values)\n",
        "\n",
        "        ## update critic\n",
        "        critic_loss = F.smooth_l1_loss(returns, torch.stack(values))\n",
        "        critic_loss.backward()\n",
        "\n",
        "        critic_optimizer.step()\n",
        "        critic_optimizer.zero_grad()\n",
        "\n",
        "        ## update actor\n",
        "        # detach advantage to update the 2nd network\n",
        "        advantages = returns - torch.Tensor(values)\n",
        "        actor_loss = (-torch.stack(log_probs) * advantages.detach()).mean()\n",
        "        actor_loss.backward()\n",
        "\n",
        "        actor_optimizer.step()\n",
        "        actor_optimizer.zero_grad()\n",
        "\n",
        "        ## If done next step them reset env\n",
        "        if log:\n",
        "            wandb.log({\n",
        "                \"actor_loss\": actor_loss,\n",
        "                \"critic_loss\": critic_loss,\n",
        "                \"epoch\": epoch,\n",
        "            })\n",
        "            if record_vids and epoch % num_episodes_to_vid == 0:\n",
        "                    record_video(env, actor, \"/content/out.mp4\")\n",
        "\n",
        "a_lr = 1e-3\n",
        "c_lr = 1e-3\n",
        "gamma = 0.99\n",
        "n_epochs = 1\n",
        "max_episode_steps = 500\n",
        "\n",
        "log = False\n",
        "record_vids = False\n",
        "\n",
        "env_id = \"CartPole-v1\"\n",
        "\n",
        "train_episodic(env_id)"
      ],
      "metadata": {
        "id": "ujkB8schA40v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23c3c43-505b-462a-b541-d1809ff67fdb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:82: UserWarning: Using a target size (torch.Size([25, 1, 1])) that is different to the input size (torch.Size([25])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_lr = 1e-3\n",
        "c_lr = 1e-3\n",
        "gamma = 0.99\n",
        "n_epochs = 2000\n",
        "max_episode_steps = 500\n",
        "\n",
        "log = True\n",
        "num_episodes_to_vid = 100\n",
        "record_vids = True\n",
        "\n",
        "# env_id = \"CartPole-v1\"\n",
        "env_id = \"Pixelcopter-PLE-v0\"\n",
        "\n",
        "train_episodic(env_id)"
      ],
      "metadata": {
        "id": "HZDSozFcgL7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### buffer\n",
        "implementation with buffer to backward calculate advantage"
      ],
      "metadata": {
        "id": "PaHdClSccX_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCriticNetwork(nn.Module):\n",
        "    def __init__(self, num_obs, num_act):\n",
        "        super(ActorCriticNetwork, self).__init__()\n",
        "\n",
        "        self.l1_actor = nn.Linear(num_obs, 64)\n",
        "        self.l2_actor = nn.Linear(64, 64)\n",
        "        self.l3_actor = nn.Linear(64, num_act)\n",
        "\n",
        "        self.l1_critic = nn.Linear(num_obs, 64)\n",
        "        self.l2_critic = nn.Linear(64, 64)\n",
        "        self.l3_critic = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_actor = self.l1_actor(x)\n",
        "        x_actor = F.relu(x_actor)\n",
        "\n",
        "        x_actor = self.l2_actor(x_actor)\n",
        "        x_actor = F.relu(x_actor)\n",
        "        action_scores = self.l3_actor(x_actor)\n",
        "        action_probs = F.softmax(action_scores, dim=1)\n",
        "\n",
        "        x_critic = self.l1_critic(x)\n",
        "        x_critic = F.relu(x_critic)\n",
        "\n",
        "        x_critic = self.l2_critic(x_critic)\n",
        "        x_critic = F.relu(x_critic)\n",
        "        \n",
        "        value = self.l3_critic(x_critic)\n",
        "\n",
        "        return action_probs, value\n",
        "\n",
        "    def act(self, state):\n",
        "        probs, value = self.forward(state)\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action, m.log_prob(action), value"
      ],
      "metadata": {
        "id": "QFCPrwsAeUDJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with a buffer\n",
        "I have a feeling that this doesn't work"
      ],
      "metadata": {
        "id": "w9Z25PaaPlO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_buffer(env_id):\n",
        "    if log: \n",
        "        name = \"a2c_\" + env_id\n",
        "        wandb.init(project=name)\n",
        "\n",
        "    env = gym.make(env_id)\n",
        "    reward = 0\n",
        "    done = False\n",
        "\n",
        "    actor, critic = make_networks(env)\n",
        "\n",
        "    if log:\n",
        "        wandb.watch((actor, critic), log_freq=1)\n",
        "\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=a_lr)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=c_lr)\n",
        "\n",
        "    num_episodes = 1\n",
        "    episode_steps = 1\n",
        "    episode_reward = 0\n",
        "\n",
        "    # (next_state)\n",
        "    #      o\n",
        "    next_state = torch.from_numpy(env.reset())\n",
        "    next_done = torch.Tensor([False])\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        dones = torch.zeros((steps_per_epoch, 1))\n",
        "        actions = torch.zeros((steps_per_epoch, 1), dtype=int)\n",
        "        states = torch.zeros((steps_per_epoch, env.observation_space.shape[0]))\n",
        "        rewards = torch.zeros((steps_per_epoch, 1))\n",
        "        logprobs = torch.zeros((steps_per_epoch, 1))\n",
        "\n",
        "        for step in range(0, steps_per_epoch):\n",
        "            #      (state, done)\n",
        "            #  (-->)     o\n",
        "            states[step] = next_state\n",
        "            dones[step] = next_done\n",
        "\n",
        "            #      (state, done)  r,a  (next_state, next_done)\n",
        "            #  (-->)  o ----------------------> o\n",
        "            actions[step], logprobs[step] = actor.act(states[step].float().unsqueeze(0))\n",
        "            next_state, rewards[step], next_done_np, _ = env.step(actions[step].detach().item())\n",
        "            next_state, next_done = torch.from_numpy(next_state), torch.Tensor([next_done_np])\n",
        "\n",
        "            episode_reward += rewards[step]\n",
        "            episode_steps += 1\n",
        "\n",
        "            if next_done or episode_steps > max_episode_steps:\n",
        "                if log:\n",
        "                    wandb.log({\n",
        "                        \"episode_steps\": episode_steps,\n",
        "                        \"episode_reward\": episode_reward,\n",
        "                        \"num_epochs\": epoch,\n",
        "                        \"num_episodes\": num_episodes,\n",
        "                    })\n",
        "\n",
        "                num_episodes += 1\n",
        "                episode_reward = 0\n",
        "                episode_steps = 1\n",
        "                next_state = torch.from_numpy(env.reset())\n",
        "                next_done = torch.Tensor([True])\n",
        "            elif dones[step]:\n",
        "                next_done = torch.Tensor([False])\n",
        "\n",
        "        # for i in range(steps_per_epoch):\n",
        "        #     print(\"[{}] d: {} r: {} state: {} a: {}\".format(\n",
        "        #         i, \n",
        "        #         dones[i], \n",
        "        #         rewards[i],\n",
        "        #         states[i],\n",
        "        #         actions[i]))\n",
        "\n",
        "        returns = torch.zeros((steps_per_epoch, 1))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_return = critic(next_state)\n",
        "            mask = 1 - next_done\n",
        "            for i in reversed(range(steps_per_epoch)):\n",
        "                if i < steps_per_epoch - 1:\n",
        "                    mask = 1 - dones[i + 1]\n",
        "                    next_return = returns[i+1]\n",
        "                returns[i] = rewards[i] + mask*gamma*next_return\n",
        "\n",
        "        # for i in range(steps_per_epoch):\n",
        "        #     print(\"[{}] d: {} r: {} R: {}\".format(\n",
        "        #         i, \n",
        "        #         dones[i], \n",
        "        #         rewards[i],\n",
        "        #         returns[i]))\n",
        "\n",
        "        values = critic(states).float()\n",
        "\n",
        "        ## update critic\n",
        "        critic_loss = F.smooth_l1_loss(returns, values).sum()\n",
        "        critic_loss.backward()\n",
        "\n",
        "        critic_optimizer.step()\n",
        "        critic_optimizer.zero_grad()\n",
        "\n",
        "        ## update actor\n",
        "        _, logprobs = actor.act(states.float().unsqueeze(0))\n",
        "        advantages = returns - values.detach()\n",
        "        actor_loss = (-logprobs * advantages).sum()\n",
        "        actor_loss.backward()\n",
        "\n",
        "        actor_optimizer.step()\n",
        "        actor_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        ## If done next step them reset env\n",
        "        if log:\n",
        "            wandb.log({\n",
        "                \"actor_loss\": actor_loss,\n",
        "                \"critic_loss\": critic_loss,\n",
        "                \"epoch\": epoch,\n",
        "            })\n",
        "            if record_vids and epoch % num_episodes_to_vid == 0:\n",
        "                    record_video(env, actor, \"/content/out.mp4\")\n",
        "\n",
        "a_lr = 1e-3\n",
        "c_lr = 1e-3\n",
        "gamma = 0.99\n",
        "n_epochs = 1\n",
        "steps_per_epoch = 40\n",
        "max_episode_steps = 500\n",
        "\n",
        "log = False\n",
        "num_episodes_to_vid = 100\n",
        "record_vids = False\n",
        "\n",
        "train_buffer(\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "wT-w7nFiYr28"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare to stable baselines 3 a2c implementation"
      ],
      "metadata": {
        "id": "egyapJIA0cIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### My code\n"
      ],
      "metadata": {
        "id": "-SWwTch35raI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a_lr = 1e-3\n",
        "c_lr = 1e-3\n",
        "gamma = 0.99\n",
        "n_epochs = 200\n",
        "steps_per_epoch = 128\n",
        "max_episode_steps = 500\n",
        "\n",
        "log = True\n",
        "num_episodes_to_vid = 100\n",
        "record_vids = True\n",
        "\n",
        "env_id = \"CartPole-v1\"\n",
        "# env_id = \"Pixelcopter-PLE-v0\"\n",
        "\n",
        "train_buffer(env_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459,
          "referenced_widgets": [
            "b99dc90b01c44cdd9b675b17064197f5",
            "a6ca302d641345dcaa651466db06d88a",
            "5fdbbafcfa7940baba346d02788de255",
            "1318eab8f2b84767ab4e95a26f433d2c",
            "e5a30f428a9341b8b4c2f656bb5b28c4",
            "6c2130f0b8a348b59af0949a751a4258",
            "90c6e4c17ef84ab9b5074c71a29e4fea",
            "3faf188a6dda410d81de70cb5880c6e1"
          ]
        },
        "id": "_8mClsHJgxHE",
        "outputId": "d5692e3b-9684-439f-dc7d-d68fc98a453a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:17lpoj6m) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b99dc90b01c44cdd9b675b17064197f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>█▇▅▆▅▃▃▂▂▂▃▃▁▁▁▂▃▂▂▂▁▂▂▃▂▁▁▂▂▃▂▃▂▁▂▂▁▂▂▁</td></tr><tr><td>critic_loss</td><td>█▇▅▇▅▄▄▃▄▃▅▄▃▂▂▂▃▂▂▂▁▂▃▃▂▂▁▁▂▄▂▃▁▁▂▂▁▁▂▁</td></tr><tr><td>episode_reward</td><td>▅▃▃▂▃▃▆▄▅▅▂▄▂▂▂▃▂▂▅▃▅▂▂▂▄▄▂▂▄▂▃██▂▅▇▁▁█▂</td></tr><tr><td>episode_steps</td><td>▅▃▃▂▃▃▆▄▅▅▂▄▂▂▂▃▂▂▅▃▅▂▂▂▄▄▂▂▄▂▃██▂▅▇▁▁█▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>num_episodes</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>num_epochs</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>actor_loss</td><td>-2557.83154</td></tr><tr><td>critic_loss</td><td>0.68763</td></tr><tr><td>episode_reward</td><td>8.0</td></tr><tr><td>episode_steps</td><td>9</td></tr><tr><td>epoch</td><td>199</td></tr><tr><td>num_episodes</td><td>1753</td></tr><tr><td>num_epochs</td><td>199</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">copper-cloud-38</strong>: <a href=\"https://wandb.ai/jefsnacker/a2c_CartPole-v1/runs/17lpoj6m\" target=\"_blank\">https://wandb.ai/jefsnacker/a2c_CartPole-v1/runs/17lpoj6m</a><br/>Synced 5 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220815_064235-17lpoj6m/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:17lpoj6m). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220815_075441-y30raom0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/jefsnacker/a2c_CartPole-v1/runs/y30raom0\" target=\"_blank\">logical-dream-39</a></strong> to <a href=\"https://wandb.ai/jefsnacker/a2c_CartPole-v1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:80: UserWarning: Using a target size (torch.Size([21, 1, 1])) that is different to the input size (torch.Size([21])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SB3"
      ],
      "metadata": {
        "id": "gIOVbU19MKVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = \"AntBulletEnv-v0\"\n",
        "env = gym.make(env_id)\n",
        "\n",
        "print(env.observation_space.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd2KHklqpC_I",
        "outputId": "4beb15cf-1e36-4dda-abda-0b8604cc7aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 1000000\n",
        "a_lr = 1e-3\n",
        "c_lr = 1e-3\n",
        "gamma = 0.99\n",
        "max_episode_steps = 500\n",
        "\n",
        "log = True\n",
        "log_rate = 1\n",
        "num_episodes_to_vid = 500\n",
        "record_vids = True\n",
        "\n",
        "# env_id = \"CartPole-v1\"\n",
        "env_id = \"Pixelcopter-PLE-v0\"\n",
        "\n",
        "train(env_id)"
      ],
      "metadata": {
        "id": "pweeYz4C0a_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stable baselines 3 implementation"
      ],
      "metadata": {
        "id": "MKY9X8ru5uaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import A2C\n",
        "\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
        "\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "\n",
        "env_id = \"CartPole-v1\"\n",
        "# env_id = \"Pixelcopter-PLE-v0\"\n",
        "policy = 'MlpPolicy'\n",
        "\n",
        "config = {\n",
        "    \"env_id\": env_id,\n",
        "    \"policy\": policy,\n",
        "}\n",
        "record_video_every_n_steps = 50000\n",
        "total_timesteps = 400000\n",
        "\n",
        "\n",
        "## Set up logging\n",
        "name = \"a2c_\" + env_id\n",
        "run = wandb.init(project=name, \n",
        "                 config=config,\n",
        "                 sync_tensorboard=True, # auto-upload sb3's tensorboard metrics\n",
        "                 monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
        "                 save_code=True)\n",
        "\n",
        "## Make the environment\n",
        "def make_env():\n",
        "    env = gym.make(config[\"env_id\"])\n",
        "    env = Monitor(env)  # record stats such as returns\n",
        "    return env\n",
        "\n",
        "env = DummyVecEnv([make_env] * 1) # 1 simulation\n",
        "env = VecVideoRecorder(\n",
        "    env, \n",
        "    f\"videos/{run.id}\", \n",
        "    record_video_trigger=lambda x: x % record_video_every_n_steps == 0, \n",
        "    video_length=200\n",
        ")\n",
        "\n",
        "# Custom actor (pi) and value function (vf) networks\n",
        "# of two layers of size 32 each with Relu activation function\n",
        "policy_kwargs = dict(activation_fn=torch.nn.ReLU,\n",
        "                     net_arch=[dict(pi=[128, 256], vf=[128, 256])])\n",
        "# Create the agent\n",
        "model = A2C(\"MlpPolicy\", env_id, policy_kwargs=policy_kwargs, verbose=1)\n",
        "\n",
        "\n",
        "## Make the model\n",
        "model = A2C(\n",
        "    policy = 'MlpPolicy',\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    env = env,\n",
        "    n_steps = 50000,\n",
        "    # learning_rate=linear_schedule(init_learning_rate),\n",
        "    # batch_size = batch_size,\n",
        "    tensorboard_log=f\"runs/{run.id}\"\n",
        ") \n",
        "\n",
        "## Train!\n",
        "model.learn(\n",
        "    total_timesteps=total_timesteps,\n",
        "    callback=WandbCallback(\n",
        "        verbose=2,\n",
        "        model_save_path=f\"models/{run.id}\"\n",
        "    )\n",
        ")\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "zhcU4ju_009W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}