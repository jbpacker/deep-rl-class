{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ppo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mTecbeNJuYR3",
        "vPFtQle1ubaY",
        "HYRj4bFDukGY",
        "H-RPq3LNufay"
      ],
      "authorship_tag": "ABX9TyPwUUd1BfItFyIW8/XHUzoI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbpacker/deep-rl-class/blob/main/unit8/ppo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PPO\n",
        "\n",
        "resources\n",
        "* [huggingface deep rl class readme](https://github.com/huggingface/deep-rl-class/tree/main/unit8)\n",
        "* [course example code](https://github.com/huggingface/deep-rl-class/blob/main/unit8/unit8.ipynb)\n",
        "* [course ppo chapter](https://huggingface.co/blog/deep-rl-ppo)\n",
        "* [cleanrl ppo](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py)"
      ],
      "metadata": {
        "id": "_dxsdQZXtxF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "knhCa6S4uWyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installs"
      ],
      "metadata": {
        "id": "mTecbeNJuYR3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aMLLKnQmtwjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42f6d6ca-b767-44fe-a2c4-4d8b48f4994f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.11).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.21.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.5)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.12.0+cu113)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: gym==0.21 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.21.0)\n",
            "Requirement already satisfied: ale-py==0.7.4 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.7.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.6.0.66)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.8.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.4.2)\n",
            "Requirement already satisfied: protobuf~=3.19.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.19.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.9.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (4.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py==0.7.4->stable-baselines3[extra]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py==0.7.4->stable-baselines3[extra]) (3.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.47.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2022.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
            "  Cloning https://github.com/ntasfi/PyGame-Learning-Environment.git to /tmp/pip-req-build-ktqesvgq\n",
            "  Running command git clone -q https://github.com/ntasfi/PyGame-Learning-Environment.git /tmp/pip-req-build-ktqesvgq\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ple==0.0.1) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from ple==0.0.1) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/qlan3/gym-games.git\n",
            "  Cloning https://github.com/qlan3/gym-games.git to /tmp/pip-req-build-9inb8o6i\n",
            "  Running command git clone -q https://github.com/qlan3/gym-games.git /tmp/pip-req-build-9inb8o6i\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from gym-games==1.0.4) (1.21.6)\n",
            "Requirement already satisfied: MinAtar>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from gym-games==1.0.4) (1.0.10)\n",
            "Requirement already satisfied: gym>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from gym-games==1.0.4) (0.21.0)\n",
            "Requirement already satisfied: setuptools>=41.0.1 in /usr/local/lib/python3.7/dist-packages (from gym-games==1.0.4) (57.4.0)\n",
            "Requirement already satisfied: pygame>=1.9.6 in /usr/local/lib/python3.7/dist-packages (from gym-games==1.0.4) (2.1.2)\n",
            "Requirement already satisfied: ple>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from gym-games==1.0.4) (0.0.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.13.0->gym-games==1.0.4) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym>=0.13.0->gym-games==1.0.4) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym>=0.13.0->gym-games==1.0.4) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym>=0.13.0->gym-games==1.0.4) (4.1.1)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.3.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (2022.1)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (0.11.0)\n",
            "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (3.2.2)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (0.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.4.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.7.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (3.0.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from ple>=0.0.1->gym-games==1.0.4) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.12.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface_hub) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.13.1)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.19.4)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.9.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.7/dist-packages (0.4.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml==6.0 in /usr/local/lib/python3.7/dist-packages (6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "\n",
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(500, 500))\n",
        "virtual_display.start()\n",
        "\n",
        "!pip install pybullet\n",
        "!pip install gym\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
        "!pip install git+https://github.com/qlan3/gym-games.git\n",
        "!pip install huggingface_hub\n",
        "!pip install wandb\n",
        "!pip install imageio-ffmpeg\n",
        "\n",
        "!pip install pyyaml==6.0 # avoid key error metadata\n",
        "\n",
        "!pip install pyglet # Virtual Screen"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "vPFtQle1ubaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import wandb\n",
        "\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import gym_pygame\n",
        "\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "\n",
        "import imageio"
      ],
      "metadata": {
        "id": "V70bIzFyuWl8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### device allocation"
      ],
      "metadata": {
        "id": "HYRj4bFDukGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "_ni2kfLCujy-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions\n",
        "\n"
      ],
      "metadata": {
        "id": "H-RPq3LNufay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def record_video(env, policy, out_director=\"/content/out.mp4\", fps=30):\n",
        "    images = []  \n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "    while not done:\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action, _ = policy.act(state)\n",
        "        state, reward, done, info = env.step(action.item()) # We directly put next_state = state for recording logic\n",
        "        img = env.render(mode='rgb_array')\n",
        "        images.append(img)\n",
        "        action.detach()\n",
        "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
        "    wandb.log({\"videos\": wandb.Video(out_directory, fps=fps)})\n",
        "\n",
        "# env_id = \"CartPole-v1\"\n",
        "# env = gym.make(env_id)\n",
        "# policy = PolicyNetwork(num_obs, num_act)\n",
        "# record_video(env, policy, \"/home/out.gif\", fps=30)"
      ],
      "metadata": {
        "id": "pPkyFIEEurUg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network"
      ],
      "metadata": {
        "id": "oReHLTb8uzuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCriticPolicy(nn.Module):\n",
        "    def __init__(self, num_obs, num_acts):\n",
        "        super(ActorCriticPolicy, self).__init__()\n",
        "\n",
        "        self.l1_actor = nn.Linear(num_obs, 128)\n",
        "        self.l2_actor = nn.Linear(128, 256)\n",
        "        self.l3_actor = nn.Linear(256, num_acts)\n",
        "\n",
        "        self.l1_critic = nn.Linear(num_obs, 128)\n",
        "        self.l2_critic = nn.Linear(128, 256)\n",
        "        self.l3_critic = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_actor = self.l1_actor(x)\n",
        "        x_actor = F.relu(x_actor)\n",
        "        x_actor = self.l2_actor(x_actor)\n",
        "        x_actor = F.relu(x_actor)\n",
        "        action_scores = self.l3_actor(x_actor)\n",
        "        action_probs = F.softmax(action_scores, dim=1)\n",
        "\n",
        "        x_critic = self.l1_critic(x)\n",
        "        x_critic = F.relu(x_critic)\n",
        "        x_critic = self.l2_critic(x_critic)\n",
        "        x_critic = F.relu(x_critic)\n",
        "        value = self.l3_critic(x_critic)\n",
        "\n",
        "        return action_probs, value"
      ],
      "metadata": {
        "id": "Jzm2AD4UuzTC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "YVSrd7C3LxA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### util classes"
      ],
      "metadata": {
        "id": "u6cYXGlnLzOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Buffer():\n",
        "    def __init__(self, env, batch_size, minibatch_size = None, gamma = 0.99):\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        if minibatch_size is None:\n",
        "            self.minibatch_size = batch_size\n",
        "        else:\n",
        "            self.minibatch_size = minibatch_size\n",
        "\n",
        "        assert self.batch_size % self.minibatch_size == 0, \"batch size must be evenly divisible by minibatch size\"\n",
        "\n",
        "        self.num_states = env.observation_space.shape[0]\n",
        "        self.num_actions = env.action_space.n\n",
        "        \n",
        "        self.reset()\n",
        "    \n",
        "    def add(self, state, next_state, action, action_prob, reward, done, value):\n",
        "        self.states[self.add_idx] = state\n",
        "        self.next_states[self.add_idx] = next_state\n",
        "        self.actions[self.add_idx] = action\n",
        "        self.action_probs[self.add_idx] = action_prob\n",
        "        self.values[self.add_idx] = value\n",
        "        self.rewards[self.add_idx] = reward\n",
        "        self.dones[self.add_idx] = done\n",
        "\n",
        "        self.add_idx += 1\n",
        "        assert self.add_idx <= self.batch_size, \"adding too many samples to buffer!\"\n",
        "        assert len(self) <= self.batch_size, \"adding too many samples to buffer!\"\n",
        "\n",
        "    def reset(self):\n",
        "        self.states = np.zeros((self.batch_size, self.num_states))\n",
        "        self.next_states = np.zeros((self.batch_size, self.num_states))\n",
        "        self.actions = np.zeros(self.batch_size)\n",
        "        self.action_probs = np.zeros((self.batch_size, self.num_actions))\n",
        "        self.values = np.zeros(self.batch_size)\n",
        "        self.rewards = np.zeros(self.batch_size)\n",
        "        self.dones = np.zeros(self.batch_size, dtype=bool)\n",
        "        self.advantages = np.zeros(self.batch_size)\n",
        "\n",
        "        self.add_idx = 0\n",
        "\n",
        "        self.shuffled_idxs = np.zeros(batch_size, dtype=int)\n",
        "        self.minibatch_idxs = np.zeros(minibatch_size, dtype=int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.states)\n",
        "\n",
        "    def calculate_advantages(self, policy, next_state):\n",
        "        _, next_value = policy(torch.from_numpy(next_state).float().unsqueeze(0))\n",
        "        next_value = next_value.detach().numpy()\n",
        "        for i in reversed(range(len(self))):\n",
        "            mask = 1 - self.dones[i]\n",
        "            if i < len(self) - 1:\n",
        "                next_value = self.values[i + 1]\n",
        "            advantage = self.rewards[i] + self.gamma * mask * next_value - self.values[i]\n",
        "            self.advantages[i] = advantage\n",
        "\n",
        "        assert(self.advantages.shape[0] == len(self)), \"final adv sizes don't match (batch size: {} adv size {})\".format(len(self), self.advantages.shape[0])\n",
        "\n",
        "    def num_minibatches(self):\n",
        "        return (int)(len(self) / self.minibatch_size)\n",
        "\n",
        "    def shuffle_minibatches(self):\n",
        "        self.minibatch_idx = 0\n",
        "\n",
        "        \n",
        "        self.shuffled_idxs = np.arange(len(self))\n",
        "        np.random.shuffle(self.shuffled_idxs)\n",
        "        \n",
        "    def get_minibatch_idxs(self):\n",
        "        start_idx = self.minibatch_idx * self.minibatch_size\n",
        "        end_idx = (self.minibatch_idx+1) * self.minibatch_size\n",
        "        self.minibatch_idxs = self.shuffled_idxs[start_idx:end_idx]\n",
        "        self.minibatch_idx += 1\n",
        "        return self.minibatch_idxs\n",
        "\n",
        "    def print(self):\n",
        "        for i in range(len(self)):\n",
        "            print(\"[{}] s: {} a: {} r: {} d: {}\".format(i, self.states[i], self.actions[i], self.rewards[i], self.dones[i]))\n",
        "\n",
        "## Tests\n",
        "# steps = 40\n",
        "# env = gym.make(\"CartPole-v1\")\n",
        "# b = Buffer(env, steps, 5)\n",
        "# b.reset()\n",
        "# policy = ActorCriticPolicy(env.observation_space.shape[0], env.action_space.n)\n",
        "# next_state = env.reset()\n",
        "# for i in range(steps):\n",
        "#     state = next_state\n",
        "\n",
        "#     # sample action\n",
        "#     probs, value = policy(torch.from_numpy(state).float().unsqueeze(0))\n",
        "#     action_dist = Categorical(probs)\n",
        "#     action = action_dist.sample()\n",
        "\n",
        "#     next_state, reward, done, info = env.step(action.item())\n",
        "#     b.add(state, next_state, action, probs.detach().numpy(), reward, done, value)\n",
        "\n",
        "#     if done:\n",
        "#         next_state = env.reset()\n",
        "\n",
        "## advantages\n",
        "# b.calculate_advantages(policy, next_state)\n",
        "# for i in range(len(b)):\n",
        "#     print(\"[{}] r: {} d: {} value: {} adv: {}\".format(i, b.rewards[i], b.dones[i], b.values[i], b.advantages[i]))\n",
        "\n",
        "## minibatches\n",
        "# b.shuffle_minibatches()\n",
        "# print(b.shuffled_idxs)\n",
        "# for i in range(b.num_minibatches()):\n",
        "#     print(b.get_minibatch_idxs())\n",
        "\n",
        "## data is added correctly\n",
        "# for i in range(len(b)):\n",
        "#     print(\"[{}] s: {} a: {} r: {} d: {}\".format(i, b.states[i], b.actions[i], b.rewards[i], b.dones[i]))\n",
        "# b.print()"
      ],
      "metadata": {
        "id": "TpKsemwWvygZ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RolloutGenerator():\n",
        "    def __init__(self, env, batch_size, minibatch_size, max_episode_steps, log):\n",
        "        self.log = log\n",
        "        self.max_episode_steps = max_episode_steps\n",
        "        self.buffer = Buffer(env, batch_size, minibatch_size)\n",
        "        \n",
        "        self.episode_reward = 0\n",
        "        self.episode_steps = 1\n",
        "        self.num_episodes = 1\n",
        "\n",
        "        self.next_state = env.reset()\n",
        "\n",
        "    def fill_buffer(self, env, policy):\n",
        "        self.buffer.reset()\n",
        "        for step in range(0, self.buffer.batch_size):\n",
        "            #      (state)\n",
        "            #  (-->)  o\n",
        "            state = self.next_state\n",
        "\n",
        "            # sample action\n",
        "            probs, value = policy(torch.from_numpy(state).float().unsqueeze(0))\n",
        "            action_dist = Categorical(probs)\n",
        "            action = action_dist.sample()\n",
        "\n",
        "            #      (state)  r,a  (next_state)\n",
        "            #  (-->)  o ------------> o\n",
        "            self.next_state, reward, done, info = env.step(action.item())\n",
        "\n",
        "            self.buffer.add(state, self.next_state, action, probs.detach().numpy(), reward, done, value)\n",
        "\n",
        "            self.episode_reward += reward\n",
        "            self.episode_steps += 1\n",
        "\n",
        "            # If episode is done or past max steps reset the env\n",
        "            if done or self.episode_steps > self.max_episode_steps:\n",
        "                if self.log:\n",
        "                    wandb.log({\n",
        "                        \"episode_steps\": self.episode_steps,\n",
        "                        \"episode_reward\": self.episode_reward,\n",
        "                        \"num_episodes\": self.num_episodes,\n",
        "                    })\n",
        "\n",
        "                self.num_episodes += 1\n",
        "                self.episode_reward = 0\n",
        "                self.episode_steps = 1\n",
        "                \n",
        "                # (next_state)\n",
        "                #      o\n",
        "                self.next_state = env.reset()\n",
        "        \n",
        "        self.buffer.calculate_advantages(policy, self.next_state)\n",
        "\n",
        "    def get_buffer(self):\n",
        "        return self.buffer\n",
        "\n",
        "# env = gym.make(\"CartPole-v1\")\n",
        "# policy = ActorCriticPolicy(env.observation_space.shape[0], env.action_space.n)\n",
        "# r = RolloutGenerator(env, 40, 10, 100, False)\n",
        "# r.fill_buffer(env, policy)\n",
        "# r.buffer.print()\n",
        "    "
      ],
      "metadata": {
        "id": "ua8birek0qlh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "lO036YmtL2ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env_id, log, lr, batch_size, minibatch_size, max_episode_steps, n_epochs, eps = 0.2):\n",
        "    if log: \n",
        "        name = \"ppo_\" + env_id\n",
        "        wandb.init(project=name)\n",
        "\n",
        "    env = gym.make(env_id)\n",
        "    policy = ActorCriticPolicy(env.observation_space.shape[0], env.action_space.n)\n",
        "\n",
        "    if log:\n",
        "        wandb.watch(policy, log_freq=1)\n",
        "\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
        "\n",
        "    rollout = RolloutGenerator(env, batch_size, minibatch_size, max_episode_steps, log)\n",
        "\n",
        "    # each epoch collects N steps regardless of episode length and trains\n",
        "    for epoch in range(n_epochs):\n",
        "        # this also calculates advantages\n",
        "        rollout.fill_buffer(env, policy)\n",
        "        rollout.buffer.shuffle_minibatches()\n",
        "\n",
        "        # Go thru all minibatches\n",
        "        for i in range(rollout.buffer.num_minibatches()):\n",
        "            # sample minibatch idxs from buffer\n",
        "            idxs = rollout.buffer.get_minibatch_idxs()\n",
        "\n",
        "            # Calculate r(t) = pi(a, s) / pi_old(a, s)\n",
        "            state = rollout.buffer.states[idxs]\n",
        "            input = torch.from_numpy(state).float()\n",
        "            # If a single row, then unsqueeze to make a batch of 1\n",
        "            if len(input.shape) == 1:\n",
        "                input = input.unsqueeze(0)\n",
        "\n",
        "\n",
        "            ## TODO(jef) remove this!!! hack to test policy\n",
        "            jacl = ActorCriticPolicy(env.observation_space.shape[0], env.action_space.n)\n",
        "            new_probs, new_value = jacl(input)\n",
        "\n",
        "\n",
        "            new_probs = new_probs.detach().numpy()\n",
        "            old_probs = rollout.buffer.action_probs[idxs]\n",
        "            action = rollout.buffer.actions[idxs].astype('int')\n",
        "\n",
        "            # Make a list of action probs based on selected action\n",
        "            new_action_probs = [ new_probs[index,action[index]] for index in range(len(action)) ]\n",
        "            old_action_probs = [ old_probs[index,action[index]] for index in range(len(action)) ]\n",
        "\n",
        "            # Finally do the ratio calculation with the probabilities\n",
        "            r = np.zeros(minibatch_size)\n",
        "            np.divide(new_action_probs, old_action_probs, r)\n",
        "            r = np.expand_dims(r, axis=1)\n",
        "            \n",
        "            # Find L_clip\n",
        "            advantage = rollout.buffer.advantages[idxs]\n",
        "            r_clip = np.clip(r, 1-eps, 1+eps)\n",
        "            L_clip = np.minimum(np.multiply(advantage, r),\n",
        "                                np.multiply(advantage, r_clip))\n",
        "            L_clip = L_clip.mean()\n",
        "            \n",
        "            # find L_vf\n",
        "            new_value = new_value.detach().numpy()\n",
        "            old_value = rollout.buffer.values[idxs]\n",
        "            old_value = np.expand_dims(old_value, axis=1)\n",
        "\n",
        "            # adv = r + gamma*v_next - v\n",
        "            # so target_v = adv + v = r + gamma*v_next\n",
        "            target_v = advantage + old_value\n",
        "            L_vf = ((new_value - target_v)**2).mean()\n",
        "            \n",
        "            # find L_entropy\n",
        "            c_entropy = 0\n",
        "            L_entropy = 0\n",
        "            c_vf = 1.0\n",
        "            \n",
        "            # Train\n",
        "            loss = L_clip + c_vf * L_vf + + c_entropy * L_entropy\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # If done next step them reset env\n",
        "            if log:\n",
        "                wandb.log({\n",
        "                    \"L_clip\": L_clip,\n",
        "                    \"L_vf\": L_vf,\n",
        "                    \"L_entropy\": L_entropy,\n",
        "                    \"loss\": loss,\n",
        "                })\n",
        "\n",
        "        # if record_vids and epoch % num_episodes_to_vid == 0:\n",
        "        #         record_video(env, policy, \"/content/out.mp4\")\n",
        "\n",
        "\n",
        "## TODO: policy is giving different results if input is batched!\n",
        "log = False\n",
        "batch_size = 4\n",
        "minibatch_size = 4\n",
        "max_episode_steps = 100\n",
        "n_epochs = 1\n",
        "lr = 1e-3\n",
        "eps = 0.2\n",
        "train(\"CartPole-v1\", log, lr, batch_size, minibatch_size, max_episode_steps, n_epochs, eps)"
      ],
      "metadata": {
        "id": "3TrMfAs5vwNJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "outputId": "5f66466a-01d0-44f4-fd04-429d96660c01"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_value\n",
            "(4, 1)\n",
            "[[ 0.00096625]\n",
            " [-0.00107259]\n",
            " [ 0.0015604 ]\n",
            " [-0.00095775]]\n",
            "old_value\n",
            "(4, 1)\n",
            "[[0.04354344]\n",
            " [0.04214137]\n",
            " [0.04447105]\n",
            " [0.04306226]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-609ab7cd36d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CartPole-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episode_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-97-609ab7cd36d6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env_id, log, lr, batch_size, minibatch_size, max_episode_steps, n_epochs, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_clip\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc_vf\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mL_vf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc_entropy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mL_entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'backward'"
          ]
        }
      ]
    }
  ]
}