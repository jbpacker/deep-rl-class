{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ppo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mTecbeNJuYR3",
        "vPFtQle1ubaY",
        "HYRj4bFDukGY",
        "H-RPq3LNufay"
      ],
      "authorship_tag": "ABX9TyOYnvlliaxe9BrPVNtRmQiv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ef922cef2f79450c8dac530f80a1de30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0b280169afa4361b205f03f7d11996d",
              "IPY_MODEL_4153bc7ee3c74f1e8cf39f6822f20c56"
            ],
            "layout": "IPY_MODEL_184a13b583f6442bad8b2c591123379c"
          }
        },
        "a0b280169afa4361b205f03f7d11996d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ecda597240e41179c2731d432047386",
            "placeholder": "​",
            "style": "IPY_MODEL_98f4d964325646b99ee25ee79b17992e",
            "value": "0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "4153bc7ee3c74f1e8cf39f6822f20c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b047a9ab8b884e27a54c5cc715b128da",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af929f08daac49d787ae8eff636091c3",
            "value": 1
          }
        },
        "184a13b583f6442bad8b2c591123379c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ecda597240e41179c2731d432047386": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98f4d964325646b99ee25ee79b17992e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b047a9ab8b884e27a54c5cc715b128da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af929f08daac49d787ae8eff636091c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbpacker/deep-rl-class/blob/main/unit8/ppo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PPO\n",
        "\n",
        "resources\n",
        "* [huggingface deep rl class readme](https://github.com/huggingface/deep-rl-class/tree/main/unit8)\n",
        "* [course example code](https://github.com/huggingface/deep-rl-class/blob/main/unit8/unit8.ipynb)\n",
        "* [course ppo chapter](https://huggingface.co/blog/deep-rl-ppo)\n",
        "* [cleanrl ppo](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py)"
      ],
      "metadata": {
        "id": "_dxsdQZXtxF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "knhCa6S4uWyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installs"
      ],
      "metadata": {
        "id": "mTecbeNJuYR3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aMLLKnQmtwjz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "faa69837-38a9-4fb1-a805-e5a28e21f4d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 1s (893 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 155680 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 785 kB of archives.\n",
            "After this operation, 2,271 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.11 [785 kB]\n",
            "Fetched 785 kB in 1s (1,467 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 158035 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.11_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.11) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.11) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 91.7 MB 31 kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.7.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-1.6.0-py3-none-any.whl (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.12.1+cu113)\n",
            "Collecting gym==0.21\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 74.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.5)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.6.0.66)\n",
            "Collecting protobuf~=3.19.0\n",
            "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 56.8 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.8.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Collecting ale-py==0.7.4\n",
            "  Downloading ale_py-0.7.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.9.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (4.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py==0.7.4->stable-baselines3[extra]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.10.0->ale-py==0.7.4->stable-baselines3[extra]) (4.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.47.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2022.1)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616825 sha256=87ac79afcaa68ff65cd3e86a6bdfda926d46fe44f2750fef5a6a91403c0854fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=f6c8637fa894940652e53e79f5e5733152fadd2a14da092641758b2861fa4cd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: protobuf, gym, AutoROM.accept-rom-license, autorom, stable-baselines3, ale-py\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.4 autorom-0.4.2 gym-0.21.0 protobuf-3.19.4 stable-baselines3-1.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
            "  Cloning https://github.com/ntasfi/PyGame-Learning-Environment.git to /tmp/pip-req-build-ltgmqjtx\n",
            "  Running command git clone -q https://github.com/ntasfi/PyGame-Learning-Environment.git /tmp/pip-req-build-ltgmqjtx\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ple==0.0.1) (1.21.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from ple==0.0.1) (7.1.2)\n",
            "Building wheels for collected packages: ple\n",
            "  Building wheel for ple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ple: filename=ple-0.0.1-py3-none-any.whl size=50791 sha256=9d67dd5099feea9145db15a6c9c8d96604dba62d8cd5d5f6cc9d25405cb4c4fe\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4fwodufr/wheels/cd/51/18/46ce3a7c7b4a75d9ba91594b40e028f98b2001414f6c1da798\n",
            "Successfully built ple\n",
            "Installing collected packages: ple\n",
            "Successfully installed ple-0.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/qlan3/gym-games.git\n",
            "  Cloning https://github.com/qlan3/gym-games.git to /tmp/pip-req-build-17x5n_5x\n",
            "  Running command git clone -q https://github.com/qlan3/gym-games.git /tmp/pip-req-build-17x5n_5x\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from gym-games==1.0.4) (1.21.6)\n",
            "Collecting MinAtar>=1.0.4\n",
            "  Downloading MinAtar-1.0.10-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: gym>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from gym-games==1.0.4) (0.21.0)\n",
            "Requirement already satisfied: setuptools>=41.0.1 in /usr/local/lib/python3.7/dist-packages (from gym-games==1.0.4) (57.4.0)\n",
            "Collecting pygame>=1.9.6\n",
            "  Downloading pygame-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 76.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ple>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from gym-games==1.0.4) (0.0.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.13.0->gym-games==1.0.4) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym>=0.13.0->gym-games==1.0.4) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym>=0.13.0->gym-games==1.0.4) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym>=0.13.0->gym-games==1.0.4) (4.1.1)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.3.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.15.0)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (2.8.2)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (0.11.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (3.0.9)\n",
            "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.7.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.7/dist-packages (from MinAtar>=1.0.4->gym-games==1.0.4) (2022.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from ple>=0.0.1->gym-games==1.0.4) (7.1.2)\n",
            "Building wheels for collected packages: gym-games\n",
            "  Building wheel for gym-games (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-games: filename=gym_games-1.0.4-py3-none-any.whl size=14632 sha256=7f0faa96cf836d91853e89062c6b429a2d39d1366ca1ce76a7140f5d61df0472\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-iuhd63pe/wheels/4d/32/86/19e03a5c068f41f6f2dd6f4197d893f97a27ae182a66bf598e\n",
            "Successfully built gym-games\n",
            "Installing collected packages: pygame, MinAtar, gym-games\n",
            "Successfully installed MinAtar-1.0.10 gym-games-1.0.4 pygame-2.1.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.1.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 57.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface_hub) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (1.24.3)\n",
            "Installing collected packages: pyyaml, huggingface-hub\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.1-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.19.4)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 55.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 42.3 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 15.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 16.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 75.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 65.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=105166df0323b58b76939df72a3aa7c5c450e5fd3fc41222e38fa3bf7fe5e9a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting imageio-ffmpeg\n",
            "  Downloading imageio_ffmpeg-0.4.7-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.9 MB 1.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: imageio-ffmpeg\n",
            "Successfully installed imageio-ffmpeg-0.4.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml==6.0 in /usr/local/lib/python3.7/dist-packages (6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "\n",
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(500, 500))\n",
        "virtual_display.start()\n",
        "\n",
        "!pip install pybullet\n",
        "!pip install gym\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
        "!pip install git+https://github.com/qlan3/gym-games.git\n",
        "!pip install huggingface_hub\n",
        "!pip install wandb\n",
        "!pip install imageio-ffmpeg\n",
        "\n",
        "!pip install pyyaml==6.0 # avoid key error metadata\n",
        "\n",
        "!pip install pyglet # Virtual Screen"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "vPFtQle1ubaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import wandb\n",
        "\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import gym_pygame\n",
        "\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "\n",
        "import imageio"
      ],
      "metadata": {
        "id": "V70bIzFyuWl8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### device allocation"
      ],
      "metadata": {
        "id": "HYRj4bFDukGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "_ni2kfLCujy-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions\n",
        "\n"
      ],
      "metadata": {
        "id": "H-RPq3LNufay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def record_video(env, policy, out_director=\"/content/out.mp4\", fps=30):\n",
        "    images = []  \n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "    while not done:\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action, _ = policy.act(state)\n",
        "        state, reward, done, info = env.step(action.item()) # We directly put next_state = state for recording logic\n",
        "        img = env.render(mode='rgb_array')\n",
        "        images.append(img)\n",
        "        action.detach()\n",
        "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
        "    wandb.log({\"videos\": wandb.Video(out_directory, fps=fps)})\n",
        "\n",
        "# env_id = \"CartPole-v1\"\n",
        "# env = gym.make(env_id)\n",
        "# policy = PolicyNetwork(num_obs, num_act)\n",
        "# record_video(env, policy, \"/home/out.gif\", fps=30)"
      ],
      "metadata": {
        "id": "pPkyFIEEurUg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network"
      ],
      "metadata": {
        "id": "oReHLTb8uzuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class ActorCriticPolicy(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(np.array(env.observation_space.shape).prod(), 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 1), std=1.0),\n",
        "        )\n",
        "        self.actor = nn.Sequential(\n",
        "            layer_init(nn.Linear(np.array(env.observation_space.shape).prod(), 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, env.action_space.n), std=0.01),\n",
        "        )\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        logits = self.actor(x)\n",
        "        probs = Categorical(logits=logits)\n",
        "        if action is None:\n",
        "            action = probs.sample()\n",
        "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class ActorCriticPolicy(nn.Module):\n",
        "#     def __init__(self, num_obs, num_acts):\n",
        "#         super(ActorCriticPolicy, self).__init__()\n",
        "\n",
        "#         self.l1_actor = nn.Linear(num_obs, 64)\n",
        "#         self.l2_actor = nn.Linear(64, 64)\n",
        "#         self.l3_actor = nn.Linear(64, num_acts)\n",
        "#         torch.nn.init.orthogonal_(self.l3_actor.weight, 1.0)\n",
        "\n",
        "#         self.l1_critic = nn.Linear(num_obs, 64)\n",
        "#         self.l2_critic = nn.Linear(64, 64)\n",
        "#         self.l3_critic = nn.Linear(64, 1)\n",
        "#         torch.nn.init.orthogonal_(self.l3_critic.weight, 0.1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x_actor = self.l1_actor(x)\n",
        "#         x_actor = F.relu(x_actor)\n",
        "#         x_actor = self.l2_actor(x_actor)\n",
        "#         x_actor = F.relu(x_actor)\n",
        "#         action_scores = self.l3_actor(x_actor)\n",
        "#         # action_probs = F.softmax(action_scores, dim=1)\n",
        "\n",
        "#         x_critic = self.l1_critic(x)\n",
        "#         x_critic = F.relu(x_critic)\n",
        "#         x_critic = self.l2_critic(x_critic)\n",
        "#         x_critic = F.relu(x_critic)\n",
        "#         value = self.l3_critic(x_critic)\n",
        "\n",
        "#         return action_scores, value\n",
        "\n",
        "#     def get_action_and_value(self, x, action=None):\n",
        "#         logits, value = self(x)\n",
        "#         probs = Categorical(logits=logits)\n",
        "#         if action is None:\n",
        "#             action = probs.sample()\n",
        "#         return action, probs.log_prob(action), probs.entropy(), value"
      ],
      "metadata": {
        "id": "Jzm2AD4UuzTC"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "YVSrd7C3LxA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### util classes"
      ],
      "metadata": {
        "id": "u6cYXGlnLzOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Buffer():\n",
        "    def __init__(self, env, batch_size, minibatch_size = None, gamma = 0.99, gae_lambda = 0.95):\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        if minibatch_size is None:\n",
        "            self.minibatch_size = batch_size\n",
        "        else:\n",
        "            self.minibatch_size = minibatch_size\n",
        "\n",
        "        assert self.batch_size % self.minibatch_size == 0, \"batch size must be evenly divisible by minibatch size\"\n",
        "\n",
        "        self.num_states = env.observation_space.shape[0]\n",
        "        self.num_actions = env.action_space.n\n",
        "        \n",
        "        self.reset()\n",
        "    \n",
        "    def add(self, state, action, log_prob, reward, done, value):\n",
        "        self.states[self.add_idx] = state\n",
        "        self.actions[self.add_idx] = action\n",
        "        self.log_probs[self.add_idx] = log_prob\n",
        "        self.values[self.add_idx] = value\n",
        "        self.rewards[self.add_idx] = reward\n",
        "        self.dones[self.add_idx] = done\n",
        "\n",
        "        self.add_idx += 1\n",
        "        assert self.add_idx <= self.batch_size, \"adding too many samples to buffer!\"\n",
        "        assert len(self) <= self.batch_size, \"adding too many samples to buffer!\"\n",
        "\n",
        "    def reset(self):\n",
        "        self.states = torch.zeros((self.batch_size, self.num_states))\n",
        "        self.actions = torch.zeros(self.batch_size, dtype=int)\n",
        "        self.log_probs = torch.zeros(self.batch_size)\n",
        "        self.values = torch.zeros(self.batch_size)\n",
        "        self.rewards = torch.zeros(self.batch_size)\n",
        "        self.dones = torch.zeros((self.batch_size), dtype=bool)\n",
        "\n",
        "        self.advantages = torch.zeros(self.batch_size)\n",
        "        self.returns = torch.zeros(self.batch_size)\n",
        "\n",
        "        self.add_idx = 0\n",
        "\n",
        "        self.shuffled_idxs = torch.zeros(self.batch_size, dtype=int)\n",
        "        self.minibatch_idxs = torch.zeros(self.minibatch_size, dtype=int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.states)\n",
        "\n",
        "    def calculate_advantages(self, policy, next_state, next_done):\n",
        "        with torch.no_grad():\n",
        "            next_value = policy.get_value(next_state.float().unsqueeze(0))\n",
        "            # for i in reversed(range(len(self))):\n",
        "            #     if i < len(self) - 1:\n",
        "            #         next_value = self.values[i + 1]\n",
        "            #     self.returns[i] = self.rewards[i] + self.gamma * ~self.dones[i] * next_value\n",
        "\n",
        "            # self.advantages = self.returns - self.values\n",
        "\n",
        "            lastgaelam = 0\n",
        "            for t in reversed(range(len(self))):\n",
        "                if t == len(self) - 1:\n",
        "                    nextnonterminal = 1.0 - next_done\n",
        "                    nextvalues = next_value\n",
        "                else:\n",
        "                    nextnonterminal = ~self.dones[t + 1]\n",
        "                    nextvalues = self.values[t + 1]\n",
        "                delta = self.rewards[t] + self.gamma * nextvalues * nextnonterminal - self.values[t]\n",
        "                self.advantages[t] = lastgaelam = delta + self.gamma * self.gae_lambda * nextnonterminal * lastgaelam\n",
        "            self.returns = self.advantages + self.values\n",
        "\n",
        "            assert(self.advantages.shape[0] == len(self)), \"final adv sizes don't match (batch size: {} adv size {})\".format(len(self), self.advantages.shape[0])\n",
        "\n",
        "    def num_minibatches(self):\n",
        "        return (int)(len(self) / self.minibatch_size)\n",
        "\n",
        "    def shuffle_minibatches(self):\n",
        "        self.minibatch_idx = 0\n",
        "\n",
        "        \n",
        "        self.shuffled_idxs = np.arange(len(self))\n",
        "        np.random.shuffle(self.shuffled_idxs)\n",
        "        \n",
        "    def get_minibatch_idxs(self):\n",
        "        start_idx = self.minibatch_idx * self.minibatch_size\n",
        "        end_idx = (self.minibatch_idx+1) * self.minibatch_size\n",
        "        self.minibatch_idxs = self.shuffled_idxs[start_idx:end_idx]\n",
        "        self.minibatch_idx += 1\n",
        "        return self.minibatch_idxs\n",
        "\n",
        "    def print(self):\n",
        "        for i in range(len(self)):\n",
        "            print(\"[{}] s: {} a: {} r: {} d: {}\".format(i, self.states[i], self.actions[i], self.rewards[i], self.dones[i]))\n",
        "\n",
        "## Tests\n",
        "steps = 40\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "b = Buffer(env, steps, 5)\n",
        "b.reset()\n",
        "policy = ActorCriticPolicy(env)\n",
        "next_state = torch.Tensor(env.reset())\n",
        "next_done = False\n",
        "for i in range(steps):\n",
        "    state = next_state\n",
        "    done = next_done\n",
        "\n",
        "    # sample action\n",
        "    with torch.no_grad():\n",
        "        action, log_prob, _, value = policy.get_action_and_value(state.float().unsqueeze(0))\n",
        "\n",
        "    next_state, reward, next_done, info = env.step(action.item())\n",
        "    next_state, next_done = torch.Tensor(next_state), torch.Tensor([next_done])\n",
        "    \n",
        "    b.add(state, action, log_prob, torch.Tensor([reward]), done, value)\n",
        "\n",
        "    if done:\n",
        "        next_state = torch.Tensor(env.reset())\n",
        "        next_done = False\n",
        "\n",
        "## advantages\n",
        "b.calculate_advantages(policy, next_state, next_done)\n",
        "for i in range(len(b)):\n",
        "    print(\"[{}] r: {} d: {} value: {} returns: {} adv: {}\".format(\n",
        "        i, \n",
        "        b.rewards[i], \n",
        "        b.dones[i], \n",
        "        b.values[i], \n",
        "        b.returns[i],\n",
        "        b.advantages[i]))\n",
        "\n",
        "## minibatches\n",
        "# b.shuffle_minibatches()\n",
        "# print(b.shuffled_idxs)\n",
        "# for i in range(b.num_minibatches()):\n",
        "#     print(b.get_minibatch_idxs())\n",
        "\n",
        "## data is added correctly\n",
        "# for i in range(len(b)):\n",
        "#     print(\"[{}] s: {} a: {} r: {} d: {} log_probs: {}\".format(i, b.states[i], b.actions[i], b.rewards[i], b.dones[i], b.log_probs[i]))\n",
        "# b.print()"
      ],
      "metadata": {
        "id": "TpKsemwWvygZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7c17c2-5431-4259-9507-a9e97789a203"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] r: 1.0 d: False value: -0.03769344463944435 returns: 14.477620124816895 adv: 14.515313148498535\n",
            "[1] r: 1.0 d: False value: -0.03687521070241928 returns: 14.3322114944458 adv: 14.369086265563965\n",
            "[2] r: 1.0 d: False value: -0.03091050684452057 returns: 14.177289009094238 adv: 14.208199501037598\n",
            "[3] r: 1.0 d: False value: -0.029574740678071976 returns: 14.012495994567871 adv: 14.042070388793945\n",
            "[4] r: 1.0 d: False value: -0.02743854746222496 returns: 13.837164878845215 adv: 13.864603042602539\n",
            "[5] r: 1.0 d: False value: -0.022808128967881203 returns: 13.650497436523438 adv: 13.67330551147461\n",
            "[6] r: 1.0 d: False value: -0.019212141633033752 returns: 13.45183277130127 adv: 13.471044540405273\n",
            "[7] r: 1.0 d: False value: -0.011899489909410477 returns: 13.240213394165039 adv: 13.252113342285156\n",
            "[8] r: 1.0 d: False value: -0.0001958245411515236 returns: 13.014592170715332 adv: 13.014787673950195\n",
            "[9] r: 1.0 d: False value: 0.005183728411793709 returns: 12.774413108825684 adv: 12.7692289352417\n",
            "[10] r: 1.0 d: False value: 0.018514633178710938 returns: 12.51833724975586 adv: 12.499822616577148\n",
            "[11] r: 1.0 d: False value: 0.024369042366743088 returns: 12.245753288269043 adv: 12.221384048461914\n",
            "[12] r: 1.0 d: False value: 0.03977639228105545 returns: 11.955113410949707 adv: 11.915336608886719\n",
            "[13] r: 1.0 d: False value: 0.056755244731903076 returns: 11.645191192626953 adv: 11.588436126708984\n",
            "[14] r: 1.0 d: False value: 0.06583976745605469 returns: 11.315184593200684 adv: 11.249344825744629\n",
            "[15] r: 1.0 d: False value: 0.08389252424240112 returns: 10.963351249694824 adv: 10.8794584274292\n",
            "[16] r: 1.0 d: False value: 0.10036779195070267 returns: 10.588391304016113 adv: 10.48802375793457\n",
            "[17] r: 1.0 d: False value: 0.11308392137289047 returns: 10.189041137695312 adv: 10.075957298278809\n",
            "[18] r: 1.0 d: False value: 0.13295988738536835 returns: 9.763381004333496 adv: 9.630420684814453\n",
            "[19] r: 1.0 d: False value: 0.1452561318874359 returns: 9.310144424438477 adv: 9.164888381958008\n",
            "[20] r: 1.0 d: False value: 0.1609060913324356 returns: 8.827409744262695 adv: 8.66650390625\n",
            "[21] r: 1.0 d: False value: 0.17203646898269653 returns: 8.313549995422363 adv: 8.14151382446289\n",
            "[22] r: 1.0 d: False value: 0.1778108775615692 returns: 7.766878128051758 adv: 7.589067459106445\n",
            "[23] r: 1.0 d: False value: 0.17884385585784912 returns: 7.185566425323486 adv: 7.006722450256348\n",
            "[24] r: 1.0 d: False value: 0.1765790581703186 returns: 6.56759786605835 adv: 6.391018867492676\n",
            "[25] r: 1.0 d: False value: 0.17261891067028046 returns: 5.910742282867432 adv: 5.738123416900635\n",
            "[26] r: 1.0 d: False value: 0.16830205917358398 returns: 5.212558746337891 adv: 5.044256687164307\n",
            "[27] r: 1.0 d: False value: 0.16455122828483582 returns: 4.470402240753174 adv: 4.305850982666016\n",
            "[28] r: 1.0 d: False value: 0.16189002990722656 returns: 3.681434154510498 adv: 3.5195441246032715\n",
            "[29] r: 1.0 d: False value: 0.1776702105998993 returns: 2.841721773147583 adv: 2.6640515327453613\n",
            "[30] r: 1.0 d: False value: 0.16938205063343048 returns: 1.949321985244751 adv: 1.779939889907837\n",
            "[31] r: 1.0 d: False value: 0.1782209724187851 returns: 1.0 adv: 0.8217790126800537\n",
            "[32] r: 0.0 d: True value: 0.16633129119873047 returns: 5.456761360168457 adv: 5.290430068969727\n",
            "[33] r: 1.0 d: False value: -0.048382632434368134 returns: 5.804525375366211 adv: 5.852908134460449\n",
            "[34] r: 1.0 d: False value: -0.048962563276290894 returns: 5.111056804656982 adv: 5.160019397735596\n",
            "[35] r: 1.0 d: False value: -0.057532455772161484 returns: 4.3741679191589355 adv: 4.431700229644775\n",
            "[36] r: 1.0 d: False value: -0.05863852798938751 returns: 3.5907177925109863 adv: 3.6493563652038574\n",
            "[37] r: 1.0 d: False value: -0.06843607872724533 returns: 2.7582192420959473 adv: 2.826655387878418\n",
            "[38] r: 1.0 d: False value: -0.07007423043251038 returns: 1.8731398582458496 adv: 1.9432140588760376\n",
            "[39] r: 1.0 d: False value: -0.06971589475870132 returns: 0.9320475459098816 adv: 1.0017634630203247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/cartpole.py:151: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  \"You are calling 'step()' even though this \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RolloutGenerator():\n",
        "    def __init__(self, env, batch_size, minibatch_size, max_episode_steps, log):\n",
        "        self.log = log\n",
        "        self.max_episode_steps = max_episode_steps\n",
        "        self.buffer = Buffer(env, batch_size, minibatch_size)\n",
        "        \n",
        "        self.episode_reward = 0\n",
        "        self.episode_steps = 1\n",
        "        self.num_episodes = 1\n",
        "\n",
        "        self.next_state = torch.Tensor(env.reset())\n",
        "        self.next_done = torch.Tensor([False])\n",
        "\n",
        "    def fill_buffer(self, env, policy):\n",
        "        self.buffer.reset()\n",
        "        for step in range(0, self.buffer.batch_size):\n",
        "            #      (state)\n",
        "            #  (-->)  o\n",
        "            state = self.next_state\n",
        "            done = self.next_done\n",
        "\n",
        "            #      (state)  r,a  (next_state)\n",
        "            #  (-->)  o ------------> o\n",
        "            with torch.no_grad():\n",
        "                action, log_prob, _, value = policy.get_action_and_value(state.float().unsqueeze(0))\n",
        "            next_state, reward, next_done, info = env.step(action.item())\n",
        "            self.next_state, self.next_done = torch.Tensor(next_state), torch.Tensor([next_done])\n",
        "\n",
        "            self.buffer.add(state, action, log_prob, torch.Tensor([reward]), done, value)\n",
        "\n",
        "            self.episode_reward += reward\n",
        "            self.episode_steps += 1\n",
        "\n",
        "            # If episode is done or past max steps reset the env\n",
        "            if done or self.episode_steps > self.max_episode_steps:\n",
        "                if self.log:\n",
        "                    wandb.log({\n",
        "                        \"episode_steps\": self.episode_steps,\n",
        "                        \"episode_reward\": self.episode_reward,\n",
        "                        \"num_episodes\": self.num_episodes,\n",
        "                    })\n",
        "\n",
        "                self.num_episodes += 1\n",
        "                self.episode_reward = 0\n",
        "                self.episode_steps = 1\n",
        "                \n",
        "                # (next_state)\n",
        "                #      o\n",
        "                self.next_state = torch.Tensor(env.reset())\n",
        "                self.next_done = False\n",
        "        \n",
        "        self.buffer.calculate_advantages(policy, self.next_state, self.next_done)\n",
        "\n",
        "    def get_buffer(self):\n",
        "        return self.buffer\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "policy = ActorCriticPolicy(env)\n",
        "r = RolloutGenerator(env, 20, 10, 100, False)\n",
        "r.fill_buffer(env, policy)\n",
        "r.buffer.print()\n",
        "    \n",
        "## data is added correctly\n",
        "# b = r.buffer\n",
        "# for i in range(len(b)):\n",
        "#     print(\"[{}] s: {} a: {} r: {} d: {} log_probs: {}\".format(i, b.states[i], b.actions[i], b.rewards[i], b.dones[i], b.log_probs[i]))"
      ],
      "metadata": {
        "id": "ua8birek0qlh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "151ef346-9450-479f-c5bd-b6cadba26596"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] s: tensor([ 0.0183,  0.0115, -0.0500, -0.0220]) a: 1 r: 1.0 d: False\n",
            "[1] s: tensor([ 0.0186,  0.2073, -0.0504, -0.3300]) a: 0 r: 1.0 d: False\n",
            "[2] s: tensor([ 0.0227,  0.0129, -0.0570, -0.0536]) a: 0 r: 1.0 d: False\n",
            "[3] s: tensor([ 0.0230, -0.1813, -0.0581,  0.2205]) a: 1 r: 1.0 d: False\n",
            "[4] s: tensor([ 0.0194,  0.0146, -0.0537, -0.0899]) a: 1 r: 1.0 d: False\n",
            "[5] s: tensor([ 0.0196,  0.2104, -0.0555, -0.3990]) a: 0 r: 1.0 d: False\n",
            "[6] s: tensor([ 0.0239,  0.0161, -0.0635, -0.1243]) a: 0 r: 1.0 d: False\n",
            "[7] s: tensor([ 0.0242, -0.1780, -0.0659,  0.1477]) a: 0 r: 1.0 d: False\n",
            "[8] s: tensor([ 0.0206, -0.3722, -0.0630,  0.4189]) a: 0 r: 1.0 d: False\n",
            "[9] s: tensor([ 0.0132, -0.5663, -0.0546,  0.6910]) a: 1 r: 1.0 d: False\n",
            "[10] s: tensor([ 0.0018, -0.3705, -0.0408,  0.3817]) a: 1 r: 1.0 d: False\n",
            "[11] s: tensor([-0.0056, -0.1748, -0.0332,  0.0764]) a: 1 r: 1.0 d: False\n",
            "[12] s: tensor([-0.0091,  0.0208, -0.0316, -0.2265]) a: 0 r: 1.0 d: False\n",
            "[13] s: tensor([-0.0086, -0.1739, -0.0362,  0.0560]) a: 1 r: 1.0 d: False\n",
            "[14] s: tensor([-0.0121,  0.0217, -0.0350, -0.2479]) a: 0 r: 1.0 d: False\n",
            "[15] s: tensor([-0.0117, -0.1729, -0.0400,  0.0336]) a: 1 r: 1.0 d: False\n",
            "[16] s: tensor([-0.0151,  0.0228, -0.0393, -0.2715]) a: 0 r: 1.0 d: False\n",
            "[17] s: tensor([-0.0147, -0.1717, -0.0448,  0.0086]) a: 1 r: 1.0 d: False\n",
            "[18] s: tensor([-0.0181,  0.0240, -0.0446, -0.2979]) a: 0 r: 1.0 d: False\n",
            "[19] s: tensor([-0.0176, -0.1705, -0.0505, -0.0196]) a: 0 r: 1.0 d: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "lO036YmtL2ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env_id, log, lr, batch_size, minibatch_size, max_episode_steps, n_epochs, eps = 0.2):\n",
        "    if log: \n",
        "        name = \"ppo_\" + env_id\n",
        "        wandb.init(project=name)\n",
        "\n",
        "    env = gym.make(env_id)\n",
        "    policy = ActorCriticPolicy(env)\n",
        "\n",
        "    if log:\n",
        "        wandb.watch(policy, log_freq=1)\n",
        "\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=lr, eps=1e-5)\n",
        "\n",
        "    rollout = RolloutGenerator(env, batch_size, minibatch_size, max_episode_steps, log)\n",
        "\n",
        "    # each epoch collects N steps regardless of episode length and trains\n",
        "    for epoch in range(n_epochs):\n",
        "        # this also calculates advantages\n",
        "        rollout.fill_buffer(env, policy)\n",
        "\n",
        "        for updates in range(4):\n",
        "            rollout.buffer.shuffle_minibatches()\n",
        "\n",
        "            if log:\n",
        "                wandb.log({\n",
        "                    \"epoch\": epoch,\n",
        "                })\n",
        "\n",
        "            # Go thru all minibatches\n",
        "            for i in range(rollout.buffer.num_minibatches()):\n",
        "                # sample minibatch idxs from buffer\n",
        "                idxs = rollout.buffer.get_minibatch_idxs()\n",
        "\n",
        "                #\n",
        "                # Step 1: Sample current policy for new_probs and new_value\n",
        "                #\n",
        "                state = rollout.buffer.states[idxs]\n",
        "                input = state.float()\n",
        "                # If a single row, then unsqueeze to make a batch of 1\n",
        "                if len(input.shape) == 1:\n",
        "                    input = input.unsqueeze(0)\n",
        "\n",
        "                _, new_log_prob, new_entropy, new_value = policy.get_action_and_value(input, rollout.buffer.actions[idxs])\n",
        "\n",
        "                #\n",
        "                # Step 2: Calculate L_clip\n",
        "                #\n",
        "\n",
        "                # Calculate r(t)\n",
        "                # r(t) = pi(a, s) / pi_old(a, s) \n",
        "                #      = exp(logprob(pi(a,s)) - logprob(pi_old(a, s)))\n",
        "                logratio = new_log_prob - rollout.buffer.log_probs[idxs]\n",
        "                r = logratio.exp()\n",
        "                \n",
        "                # Find policy loss\n",
        "                advantage = rollout.buffer.advantages[idxs]\n",
        "                policy_loss1 = -advantage * r\n",
        "                policy_loss2 = -advantage * torch.clamp(r, 1 - eps, 1 + eps)\n",
        "                policy_loss = torch.max(policy_loss1, policy_loss2).mean()\n",
        "                \n",
        "                #\n",
        "                # Step 3: Calculate L_vf\n",
        "                #\n",
        "                returns = rollout.buffer.returns[idxs]\n",
        "                value_loss = 0.5 * ((new_value - returns)**2).mean()\n",
        "                \n",
        "                #\n",
        "                # Step 4: Calculate L_entropy\n",
        "                #\n",
        "\n",
        "                # find L_entropy\n",
        "                c_entropy = 0.01\n",
        "                entropy_loss = new_entropy.mean()\n",
        "                c_vf = 0.5\n",
        "                \n",
        "                #\n",
        "                # Step 4: Train\n",
        "                #\n",
        "                loss = policy_loss - c_entropy * entropy_loss + c_vf * value_loss\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # If done next step them reset env\n",
        "                if log:\n",
        "                    wandb.log({\n",
        "                        \"policy_loss\": policy_loss,\n",
        "                        \"value_loss\": value_loss,\n",
        "                        \"entropy_loss\": entropy_loss,\n",
        "                        \"loss\": loss,\n",
        "                    })\n",
        "\n",
        "        # if record_vids and epoch % num_episodes_to_vid == 0:\n",
        "        #         record_video(env, policy, \"/content/out.mp4\")\n",
        "\n",
        "\n",
        "log = False\n",
        "batch_size = 8\n",
        "minibatch_size = 4\n",
        "max_episode_steps = 100\n",
        "n_epochs = 1\n",
        "lr = 1e-3\n",
        "eps = 0.2\n",
        "train(\"CartPole-v1\", log, lr, batch_size, minibatch_size, max_episode_steps, n_epochs, eps)"
      ],
      "metadata": {
        "id": "3TrMfAs5vwNJ"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log = True\n",
        "env_id = \"CartPole-v1\"\n",
        "batch_size = 128\n",
        "minibatch_size = 32\n",
        "max_episode_steps = 500\n",
        "n_epochs = 250\n",
        "lr = 2.5e-4\n",
        "eps = 0.2\n",
        "train(env_id, log, lr, batch_size, minibatch_size, max_episode_steps, n_epochs, eps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391,
          "referenced_widgets": [
            "ef922cef2f79450c8dac530f80a1de30",
            "a0b280169afa4361b205f03f7d11996d",
            "4153bc7ee3c74f1e8cf39f6822f20c56",
            "184a13b583f6442bad8b2c591123379c",
            "0ecda597240e41179c2731d432047386",
            "98f4d964325646b99ee25ee79b17992e",
            "b047a9ab8b884e27a54c5cc715b128da",
            "af929f08daac49d787ae8eff636091c3"
          ]
        },
        "id": "SRxgwW4-LWDk",
        "outputId": "64ce2e80-0deb-454f-e5b4-602b160602eb"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:jgvlpp14) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef922cef2f79450c8dac530f80a1de30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode_reward</td><td>█▂▁</td></tr><tr><td>episode_steps</td><td>█▂▁</td></tr><tr><td>num_episodes</td><td>▁▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode_reward</td><td>22.0</td></tr><tr><td>episode_steps</td><td>24</td></tr><tr><td>num_episodes</td><td>3</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">sweet-vortex-13</strong>: <a href=\"https://wandb.ai/jefsnacker/ppo_CartPole-v1/runs/jgvlpp14\" target=\"_blank\">https://wandb.ai/jefsnacker/ppo_CartPole-v1/runs/jgvlpp14</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220813_101656-jgvlpp14/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:jgvlpp14). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220813_101727-3v17cdzt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/jefsnacker/ppo_CartPole-v1/runs/3v17cdzt\" target=\"_blank\">smooth-bird-14</a></strong> to <a href=\"https://wandb.ai/jefsnacker/ppo_CartPole-v1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/cartpole.py:151: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  \"You are calling 'step()' even though this \"\n"
          ]
        }
      ]
    }
  ]
}