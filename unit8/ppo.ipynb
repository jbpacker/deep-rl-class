{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ppo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vPFtQle1ubaY",
        "HYRj4bFDukGY",
        "H-RPq3LNufay",
        "oReHLTb8uzuv"
      ],
      "authorship_tag": "ABX9TyN89H4suflxOsA+ACrpMl51",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "57580abfd5c54091a630defe96782a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec6c4ab7340d481aa89961fbb52055cc",
              "IPY_MODEL_24cc3e8517d0435e8db1e79042f55d92"
            ],
            "layout": "IPY_MODEL_c9f46127cf5a4363b9ebf345260c9fcc"
          }
        },
        "ec6c4ab7340d481aa89961fbb52055cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c7ad63991d54cba9e19e8ab5c99c990",
            "placeholder": "​",
            "style": "IPY_MODEL_3961dc98a0a24a6a8c06f63b7940d6b1",
            "value": "0.030 MB of 0.030 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "24cc3e8517d0435e8db1e79042f55d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f58344a16b7b4209bbb2a701c1bd7124",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8289a9d9d78c4592961303ed7404bc3a",
            "value": 1
          }
        },
        "c9f46127cf5a4363b9ebf345260c9fcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c7ad63991d54cba9e19e8ab5c99c990": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3961dc98a0a24a6a8c06f63b7940d6b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f58344a16b7b4209bbb2a701c1bd7124": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8289a9d9d78c4592961303ed7404bc3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbpacker/deep-rl-class/blob/main/unit8/ppo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PPO\n",
        "\n",
        "resources\n",
        "* [huggingface deep rl class readme](https://github.com/huggingface/deep-rl-class/tree/main/unit8)\n",
        "* [course example code](https://github.com/huggingface/deep-rl-class/blob/main/unit8/unit8.ipynb)\n",
        "* [course ppo chapter](https://huggingface.co/blog/deep-rl-ppo)\n",
        "* [cleanrl ppo](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py)\n",
        "\n",
        "TODO:\n",
        "* extra step() error\n",
        "* revisit advantage calculation since that's where the error was\n",
        "* check out https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/ for small improvements"
      ],
      "metadata": {
        "id": "_dxsdQZXtxF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "knhCa6S4uWyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installs"
      ],
      "metadata": {
        "id": "mTecbeNJuYR3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMLLKnQmtwjz"
      },
      "outputs": [],
      "source": [
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "\n",
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(500, 500))\n",
        "virtual_display.start()\n",
        "\n",
        "!pip install pybullet\n",
        "!pip install gym\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
        "!pip install git+https://github.com/qlan3/gym-games.git\n",
        "!pip install huggingface_hub\n",
        "!pip install wandb\n",
        "!pip install imageio-ffmpeg\n",
        "\n",
        "!pip install pyyaml==6.0 # avoid key error metadata\n",
        "\n",
        "!pip install pyglet # Virtual Screen"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "vPFtQle1ubaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import wandb\n",
        "\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import gym_pygame\n",
        "\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "\n",
        "import imageio"
      ],
      "metadata": {
        "id": "V70bIzFyuWl8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### device allocation"
      ],
      "metadata": {
        "id": "HYRj4bFDukGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "_ni2kfLCujy-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions\n",
        "\n"
      ],
      "metadata": {
        "id": "H-RPq3LNufay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def record_video(env, policy, out_directory=\"/content/out.mp4\", fps=30):\n",
        "    images = []  \n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "    while not done:\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        with torch.no_grad():\n",
        "            action, _, _, _ = policy.get_action_and_value(torch.Tensor(state).float().unsqueeze(0))\n",
        "        state, reward, done, info = env.step(action.item()) # We directly put next_state = state for recording logic\n",
        "        img = env.render(mode='rgb_array')\n",
        "        images.append(img)\n",
        "        action.detach()\n",
        "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
        "    wandb.log({\"videos\": wandb.Video(out_directory, fps=fps)})\n",
        "\n",
        "# env_id = \"CartPole-v1\"\n",
        "# env = gym.make(env_id)\n",
        "# policy = PolicyNetwork(num_obs, num_act)\n",
        "# record_video(env, policy, \"/home/out.gif\", fps=30)"
      ],
      "metadata": {
        "id": "pPkyFIEEurUg"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network"
      ],
      "metadata": {
        "id": "oReHLTb8uzuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class ActorCriticPolicy(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(np.array(env.observation_space.shape).prod(), 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 1), std=1.0),\n",
        "        )\n",
        "        self.actor = nn.Sequential(\n",
        "            layer_init(nn.Linear(np.array(env.observation_space.shape).prod(), 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, env.action_space.n), std=0.01),\n",
        "        )\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        logits = self.actor(x)\n",
        "        probs = Categorical(logits=logits)\n",
        "        if action is None:\n",
        "            action = probs.sample()\n",
        "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class ActorCriticPolicy(nn.Module):\n",
        "#     def __init__(self, num_obs, num_acts):\n",
        "#         super(ActorCriticPolicy, self).__init__()\n",
        "\n",
        "#         self.l1_actor = nn.Linear(num_obs, 64)\n",
        "#         self.l2_actor = nn.Linear(64, 64)\n",
        "#         self.l3_actor = nn.Linear(64, num_acts)\n",
        "#         torch.nn.init.orthogonal_(self.l3_actor.weight, 1.0)\n",
        "\n",
        "#         self.l1_critic = nn.Linear(num_obs, 64)\n",
        "#         self.l2_critic = nn.Linear(64, 64)\n",
        "#         self.l3_critic = nn.Linear(64, 1)\n",
        "#         torch.nn.init.orthogonal_(self.l3_critic.weight, 0.1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x_actor = self.l1_actor(x)\n",
        "#         x_actor = F.relu(x_actor)\n",
        "#         x_actor = self.l2_actor(x_actor)\n",
        "#         x_actor = F.relu(x_actor)\n",
        "#         action_scores = self.l3_actor(x_actor)\n",
        "#         # action_probs = F.softmax(action_scores, dim=1)\n",
        "\n",
        "#         x_critic = self.l1_critic(x)\n",
        "#         x_critic = F.relu(x_critic)\n",
        "#         x_critic = self.l2_critic(x_critic)\n",
        "#         x_critic = F.relu(x_critic)\n",
        "#         value = self.l3_critic(x_critic)\n",
        "\n",
        "#         return action_scores, value\n",
        "\n",
        "#     def get_action_and_value(self, x, action=None):\n",
        "#         logits, value = self(x)\n",
        "#         probs = Categorical(logits=logits)\n",
        "#         if action is None:\n",
        "#             action = probs.sample()\n",
        "#         return action, probs.log_prob(action), probs.entropy(), value"
      ],
      "metadata": {
        "id": "Jzm2AD4UuzTC"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "YVSrd7C3LxA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### util classes"
      ],
      "metadata": {
        "id": "u6cYXGlnLzOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Buffer():\n",
        "    def __init__(self, env, batch_size, minibatch_size = None, gamma = 0.99, gae_lambda = 0.95):\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        if minibatch_size is None:\n",
        "            self.minibatch_size = batch_size\n",
        "        else:\n",
        "            self.minibatch_size = minibatch_size\n",
        "\n",
        "        assert self.batch_size % self.minibatch_size == 0, \"batch size must be evenly divisible by minibatch size\"\n",
        "\n",
        "        self.num_states = env.observation_space.shape[0]\n",
        "        self.num_actions = env.action_space.n\n",
        "        \n",
        "        self.reset()\n",
        "    \n",
        "    def add(self, state, action, log_prob, reward, done, value):\n",
        "        self.states[self.add_idx] = state\n",
        "        self.actions[self.add_idx] = action\n",
        "        self.log_probs[self.add_idx] = log_prob\n",
        "        self.values[self.add_idx] = value\n",
        "        self.rewards[self.add_idx] = reward\n",
        "        self.dones[self.add_idx] = done\n",
        "\n",
        "        self.add_idx += 1\n",
        "        assert self.add_idx <= self.batch_size, \"adding too many samples to buffer!\"\n",
        "        assert len(self) <= self.batch_size, \"adding too many samples to buffer!\"\n",
        "\n",
        "    def reset(self):\n",
        "        self.states = torch.zeros((self.batch_size, self.num_states))\n",
        "        self.actions = torch.zeros(self.batch_size, dtype=int)\n",
        "        self.log_probs = torch.zeros(self.batch_size)\n",
        "        self.values = torch.zeros(self.batch_size)\n",
        "        self.rewards = torch.zeros(self.batch_size)\n",
        "        self.dones = torch.zeros(self.batch_size)\n",
        "\n",
        "        self.advantages = torch.zeros(self.batch_size)\n",
        "        self.returns = torch.zeros(self.batch_size)\n",
        "\n",
        "        self.add_idx = 0\n",
        "\n",
        "        self.shuffled_idxs = torch.zeros(self.batch_size, dtype=int)\n",
        "        self.minibatch_idxs = torch.zeros(self.minibatch_size, dtype=int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.states)\n",
        "\n",
        "    ## Calculate with advantages\n",
        "    # Advantage = gamma * V(s+1) + r - V(s)\n",
        "    # return = gamma * v(s+1) + r\n",
        "    # bellman V(s) = r + gamma*V(s+1)\n",
        "    # def calculate_advantages(self, policy, next_state, next_done):\n",
        "    #     with torch.no_grad():\n",
        "    #         next_value = policy.get_value(next_state.float().unsqueeze(0))\n",
        "    #         mask = 1 - next_done\n",
        "    #         for i in reversed(range(len(self))):\n",
        "    #             if i < len(self) - 1:\n",
        "    #                 next_value = self.returns[i+1]\n",
        "    #                 mask = 1 - self.dones[i+1]\n",
        "    #             self.returns[i] = self.rewards[i] + mask * self.gamma * next_value\n",
        "\n",
        "    #         self.advantages = self.returns - self.values\n",
        "\n",
        "    ## Calculate with GAE\n",
        "    def calculate_advantages(self, policy, next_state, next_done):\n",
        "        with torch.no_grad():\n",
        "            next_value = policy.get_value(next_state.float().unsqueeze(0))\n",
        "            # for i in reversed(range(len(self))):\n",
        "            #     if i < len(self) - 1:\n",
        "            #         next_value = self.values[i + 1]\n",
        "            #     self.returns[i] = self.rewards[i] + self.gamma * ~self.dones[i] * next_value\n",
        "\n",
        "            # self.advantages = self.returns - self.values\n",
        "\n",
        "            lastgaelam = 0\n",
        "            for t in reversed(range(len(self))):\n",
        "                if t == len(self) - 1:\n",
        "                    nextnonterminal = 1.0 - next_done\n",
        "                    nextvalues = next_value\n",
        "                else:\n",
        "                    nextnonterminal = 1.0 - self.dones[t + 1]\n",
        "                    nextvalues = self.values[t + 1]\n",
        "                delta = self.rewards[t] + self.gamma * nextvalues * nextnonterminal - self.values[t]\n",
        "                self.advantages[t] = lastgaelam = delta + self.gamma * self.gae_lambda * nextnonterminal * lastgaelam\n",
        "            self.returns = self.advantages + self.values\n",
        "\n",
        "            assert(self.advantages.shape[0] == len(self)), \"final adv sizes don't match (batch size: {} adv size {})\".format(len(self), self.advantages.shape[0])\n",
        "\n",
        "    def num_minibatches(self):\n",
        "        return (int)(len(self) / self.minibatch_size)\n",
        "\n",
        "    def shuffle_minibatches(self):\n",
        "        self.minibatch_idx = 0\n",
        "\n",
        "        \n",
        "        self.shuffled_idxs = np.arange(len(self))\n",
        "        np.random.shuffle(self.shuffled_idxs)\n",
        "        \n",
        "    def get_minibatch_idxs(self):\n",
        "        start_idx = self.minibatch_idx * self.minibatch_size\n",
        "        end_idx = (self.minibatch_idx+1) * self.minibatch_size\n",
        "        self.minibatch_idxs = self.shuffled_idxs[start_idx:end_idx]\n",
        "        self.minibatch_idx += 1\n",
        "        return self.minibatch_idxs\n",
        "\n",
        "    def print(self):\n",
        "        for i in range(len(self)):\n",
        "            print(\"[{}] s: {} a: {} r: {} d: {}\".format(i, self.states[i], self.actions[i], self.rewards[i], self.dones[i]))\n",
        "\n",
        "    def print_adv(self):\n",
        "        for i in range(len(self)):\n",
        "            print(\"[{}] r: {} d: {} value: {} returns: {} adv: {}\".format(\n",
        "                i, \n",
        "                self.rewards[i], \n",
        "                self.dones[i], \n",
        "                self.values[i], \n",
        "                self.returns[i],\n",
        "                self.advantages[i]))"
      ],
      "metadata": {
        "id": "TpKsemwWvygZ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RolloutGenerator\n",
        "\n",
        "Fills the buffer with an exact number of steps with multiple rollouts.\n",
        "\n",
        "Important:\n",
        "the buffer will be filled with the following information\n",
        "\n",
        "```\n",
        "[state, action, reward, done, value]\n",
        "```\n",
        "and the transition will look like the following for specific entries\n",
        "\n",
        "Normal\n",
        "```\n",
        "O (state, value, done)\n",
        "|\n",
        "| a, r\n",
        "v \n",
        "O (state+1, value+1, done+1)\n",
        "|\n",
        "| a+1,r+1\n",
        "v \n",
        "O\n",
        "```\n",
        "\n",
        "Terminal\n",
        "```\n",
        " O (s_f, v_f, d_f=False)\n",
        "(|)\n",
        "(|) a_f,r_f\n",
        "(v)\n",
        " O (s_1, v_1, d_1=True)\n",
        " |\n",
        " | a_1,r_1\n",
        " v\n",
        " O\n",
        " ```\n",
        "Note that `done=True` is on the first state of the new sequence. This is because any actions and rewards found past the final state won't be correct or defined with respect to the environment."
      ],
      "metadata": {
        "id": "h1xe6pYzDgKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RolloutGenerator():\n",
        "    def __init__(self, env, batch_size, minibatch_size, max_episode_steps, log):\n",
        "        self.log = log\n",
        "        self.max_episode_steps = max_episode_steps\n",
        "        self.buffer = Buffer(env, batch_size, minibatch_size)\n",
        "        \n",
        "        self.episode_reward = 0\n",
        "        self.episode_steps = 1\n",
        "        self.num_episodes = 1\n",
        "\n",
        "        self.next_state = torch.Tensor(env.reset())\n",
        "        self.next_done = torch.Tensor([False])\n",
        "\n",
        "    def fill_buffer(self, env, policy):\n",
        "        self.buffer.reset()\n",
        "        for step in range(0, self.buffer.batch_size):\n",
        "            #      (state)\n",
        "            #  (-->)  o\n",
        "            state = self.next_state\n",
        "            done = self.next_done\n",
        "\n",
        "            #      (state, done)  r,a  (next_state, next_done)\n",
        "            #  (-->)     o ---------------------> o\n",
        "            with torch.no_grad():\n",
        "                action, log_prob, _, value = policy.get_action_and_value(state.float().unsqueeze(0))\n",
        "            next_state, reward, next_done, info = env.step(action.item())\n",
        "            self.next_state, self.next_done = torch.Tensor(next_state), torch.Tensor([next_done])\n",
        "\n",
        "            self.buffer.add(state, action, log_prob, torch.Tensor([reward]), done, value)\n",
        "\n",
        "            self.episode_reward += reward\n",
        "            self.episode_steps += 1\n",
        "\n",
        "            # If episode is next_done or past max steps reset the env\n",
        "            if self.next_done or self.episode_steps > self.max_episode_steps:\n",
        "                if self.log:\n",
        "                    wandb.log({\n",
        "                        \"episode_steps\": self.episode_steps,\n",
        "                        \"episode_reward\": self.episode_reward,\n",
        "                        \"num_episodes\": self.num_episodes,\n",
        "                    })\n",
        "\n",
        "                self.num_episodes += 1\n",
        "                self.episode_reward = 0\n",
        "                self.episode_steps = 1\n",
        "                \n",
        "                # (next_state)\n",
        "                #      o\n",
        "                self.next_state = torch.Tensor(env.reset())\n",
        "                \n",
        "            if done:\n",
        "                self.next_done = torch.Tensor([False])\n",
        "        \n",
        "        self.buffer.calculate_advantages(policy, self.next_state, self.next_done)\n",
        "\n",
        "    def get_buffer(self):\n",
        "        return self.buffer\n",
        "\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "policy = ActorCriticPolicy(env)\n",
        "r = RolloutGenerator(env, 40, 10, 100, False)\n",
        "r.fill_buffer(env, policy)\n",
        "# r.buffer.print()\n",
        "    \n",
        "## data is added correctly\n",
        "b = r.buffer\n",
        "for i in range(len(b)):\n",
        "    print(\"[{}]: s: {} a: {} r: {} d: {} v: {}\".format(\n",
        "        i, \n",
        "        b.states[i], \n",
        "        b.actions[i], \n",
        "        b.rewards[i], \n",
        "        b.dones[i],\n",
        "        b.values[i]))\n",
        "    \n",
        "for i in range(len(b)):\n",
        "    print(\"[{}] r: {} d: {} value: {} returns: {} adv: {}\".format(\n",
        "        i, \n",
        "        b.rewards[i], \n",
        "        b.dones[i], \n",
        "        b.values[i], \n",
        "        b.returns[i],\n",
        "        b.advantages[i]))"
      ],
      "metadata": {
        "id": "ua8birek0qlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "lO036YmtL2ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env_id, log, lr, batch_size, minibatch_size, max_episode_steps, n_epochs, eps = 0.2):\n",
        "    if log: \n",
        "        name = \"ppo_\" + env_id\n",
        "        wandb.init(project=name)\n",
        "\n",
        "    env = gym.make(env_id)\n",
        "    policy = ActorCriticPolicy(env)\n",
        "\n",
        "    if log:\n",
        "        wandb.watch(policy, log_freq=1)\n",
        "\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=lr, eps=1e-5)\n",
        "\n",
        "    rollout = RolloutGenerator(env, batch_size, minibatch_size, max_episode_steps, log)\n",
        "\n",
        "    # each epoch collects N steps regardless of episode length and trains\n",
        "    for epoch in range(n_epochs):\n",
        "        # this also calculates advantages\n",
        "        rollout.fill_buffer(env, policy)\n",
        "\n",
        "        for updates in range(4):\n",
        "            rollout.buffer.shuffle_minibatches()\n",
        "\n",
        "            if log:\n",
        "                wandb.log({\n",
        "                    \"epoch\": epoch,\n",
        "                })\n",
        "\n",
        "            # Go thru all minibatches\n",
        "            for i in range(rollout.buffer.num_minibatches()):\n",
        "                # sample minibatch idxs from buffer\n",
        "                idxs = rollout.buffer.get_minibatch_idxs()\n",
        "\n",
        "                #\n",
        "                # Step 1: Sample current policy for new_probs and new_value\n",
        "                #\n",
        "                state = rollout.buffer.states[idxs]\n",
        "                input = state.float()\n",
        "                # If a single row, then unsqueeze to make a batch of 1\n",
        "                if len(input.shape) == 1:\n",
        "                    input = input.unsqueeze(0)\n",
        "\n",
        "                _, new_log_prob, new_entropy, new_value = policy.get_action_and_value(input, rollout.buffer.actions[idxs])\n",
        "\n",
        "                #\n",
        "                # Step 2: Calculate L_clip\n",
        "                #\n",
        "\n",
        "                # Calculate r(t)\n",
        "                # r(t) = pi(a, s) / pi_old(a, s) \n",
        "                #      = exp(logprob(pi(a,s)) - logprob(pi_old(a, s)))\n",
        "                logratio = new_log_prob - rollout.buffer.log_probs[idxs]\n",
        "                r = logratio.exp()\n",
        "                \n",
        "                # Find policy loss\n",
        "                advantage = rollout.buffer.advantages[idxs]\n",
        "                policy_loss1 = -advantage * r\n",
        "                policy_loss2 = -advantage * torch.clamp(r, 1 - eps, 1 + eps)\n",
        "                policy_loss = torch.max(policy_loss1, policy_loss2).mean()\n",
        "                \n",
        "                #\n",
        "                # Step 3: Calculate L_vf\n",
        "                #\n",
        "                returns = rollout.buffer.returns[idxs]\n",
        "                value_loss = 0.5 * ((new_value - returns)**2).mean()\n",
        "                \n",
        "                #\n",
        "                # Step 4: Calculate L_entropy\n",
        "                #\n",
        "\n",
        "                # find L_entropy\n",
        "                c_entropy = 0.01\n",
        "                entropy_loss = new_entropy.mean()\n",
        "                c_vf = 0.5\n",
        "                \n",
        "                #\n",
        "                # Step 4: Train\n",
        "                #\n",
        "                loss = policy_loss - c_entropy * entropy_loss + c_vf * value_loss\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # If done next step them reset env\n",
        "                if log:\n",
        "                    wandb.log({\n",
        "                        \"policy_loss\": policy_loss,\n",
        "                        \"value_loss\": value_loss,\n",
        "                        \"entropy_loss\": entropy_loss,\n",
        "                        \"loss\": loss,\n",
        "                    })\n",
        "\n",
        "        if record_vids and epoch % num_epochs_to_vid == 0:\n",
        "            record_video(env, policy, \"/content/out.mp4\")\n",
        "\n",
        "\n",
        "record_vids = False\n",
        "log = False\n",
        "batch_size = 8\n",
        "minibatch_size = 4\n",
        "max_episode_steps = 100\n",
        "n_epochs = 1\n",
        "lr = 1e-3\n",
        "eps = 0.2\n",
        "train(\"CartPole-v1\", log, lr, batch_size, minibatch_size, max_episode_steps, n_epochs, eps)"
      ],
      "metadata": {
        "id": "3TrMfAs5vwNJ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log = True\n",
        "record_vids = True\n",
        "num_epochs_to_vid = 100\n",
        "# env_id = \"CartPole-v1\"\n",
        "# env_id = \"LunarLander-v2\"\n",
        "env_id = \"Pixelcopter-PLE-v0\"\n",
        "batch_size = 512\n",
        "minibatch_size = 128\n",
        "max_episode_steps = 1000\n",
        "n_epochs = 500\n",
        "lr = 2.5e-4\n",
        "eps = 0.2\n",
        "train(env_id, log, lr, batch_size, minibatch_size, max_episode_steps, n_epochs, eps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443,
          "referenced_widgets": [
            "57580abfd5c54091a630defe96782a93",
            "ec6c4ab7340d481aa89961fbb52055cc",
            "24cc3e8517d0435e8db1e79042f55d92",
            "c9f46127cf5a4363b9ebf345260c9fcc",
            "0c7ad63991d54cba9e19e8ab5c99c990",
            "3961dc98a0a24a6a8c06f63b7940d6b1",
            "f58344a16b7b4209bbb2a701c1bd7124",
            "8289a9d9d78c4592961303ed7404bc3a"
          ]
        },
        "id": "SRxgwW4-LWDk",
        "outputId": "0cd78c4c-da84-4fce-ee7c-b3d132f99085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:2az5wqd0) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57580abfd5c54091a630defe96782a93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>entropy_loss</td><td>██▇▇▇▇▇▆▆▅▅▅▄▃▂▁</td></tr><tr><td>episode_reward</td><td>▅▁▅▁▁▁▅▁▁▁▁▅▁▁▁▁▁▅▁▁▁▁▁▁▁▅▁▁▁▁█▅▅▁▁▁▁▁▁█</td></tr><tr><td>episode_steps</td><td>▅▂▅▃▂▁▅▂▂▄▁▅▂▂▃▃▂▅▂▃▃▃▂▃▂▆▂▃▄▂█▅▅▂▂▂▄▃▂█</td></tr><tr><td>epoch</td><td>▁▁▁▁</td></tr><tr><td>loss</td><td>▇█▆▆▇▄▄▃▃▅▂▂▁▂▁▂</td></tr><tr><td>num_episodes</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>policy_loss</td><td>▄▆▃▃█▃▃▁▃█▁▃▁▅▃▆</td></tr><tr><td>value_loss</td><td>██▆▆▇▅▄▄▃▄▂▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>entropy_loss</td><td>0.69133</td></tr><tr><td>episode_reward</td><td>-3.0</td></tr><tr><td>episode_steps</td><td>17</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>5.18056</td></tr><tr><td>num_episodes</td><td>66</td></tr><tr><td>policy_loss</td><td>3.48082</td></tr><tr><td>value_loss</td><td>3.4133</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">gallant-bush-3</strong>: <a href=\"https://wandb.ai/jefsnacker/ppo_Pixelcopter-PLE-v0/runs/2az5wqd0\" target=\"_blank\">https://wandb.ai/jefsnacker/ppo_Pixelcopter-PLE-v0/runs/2az5wqd0</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220814_114936-2az5wqd0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:2az5wqd0). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220814_115228-355paaad</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/jefsnacker/ppo_Pixelcopter-PLE-v0/runs/355paaad\" target=\"_blank\">hopeful-haze-4</a></strong> to <a href=\"https://wandb.ai/jefsnacker/ppo_Pixelcopter-PLE-v0\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}