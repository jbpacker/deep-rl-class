{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOeEba/AUGO3gfu86PlUyKM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbpacker/deep-rl-class/blob/main/unit3/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self made DQN implementation\n",
        "\n",
        "Weights and Biases: https://wandb.ai/jefsnacker/DQN?workspace=user-jefsnacker\n",
        "\n",
        "next steps:\n",
        "1. eval mode that runs every n training steps\n",
        "1. video replay for evaluation\n",
        "1. longer run to see if it's actually training correctly\n",
        "\n",
        "### Resources\n",
        "\n",
        "PPO implementation details: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
        "\n",
        "Great discription of DQN can be found here: https://huggingface.co/blog/deep-rl-dqn\n",
        "\n",
        "walkthrough code: https://pylessons.com/CartPole-DDQN"
      ],
      "metadata": {
        "id": "-SjRVhWwCBOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "Install relevant libraries and initialize virtual display"
      ],
      "metadata": {
        "id": "w9u09vSAC6Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pyglet==1.5.1 \n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "\n",
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "clZutrX2C1xs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/openai/gym.git # We install gym using git since Taxi-v3 \"rgb_array version\" is not on PyPi release\n",
        "!pip install pygame\n",
        "!pip install numpy\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "NkS-0gJvCBE9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "# import imageio\n",
        "# import os\n",
        "# import scipy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "\n",
        "\n",
        "# import pickle5 as pickle\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "elnd2-RUCsm-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment\n",
        "\n",
        "discrete action space: https://github.com/openai/gym/blob/master/gym/spaces/discrete.py"
      ],
      "metadata": {
        "id": "9TAa_CxN8VnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "hAy1Velu8UKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscreteModel(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(env.observation_space.shape[0], 512)\n",
        "        self.l2 = nn.Linear(512, 256)\n",
        "        self.l3 = nn.Linear(256, 64)\n",
        "        self.l4 = nn.Linear(64, env.action_space.n)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = F.relu(self.l3(x))\n",
        "        x = self.l4(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "G67ZFYJH8T3X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor([[-0.0140, -0.1768,  0.0392,  0.2849],\n",
        "        [-0.0175, -0.3725,  0.0449,  0.5897],\n",
        "        [-0.0250, -0.5682,  0.0567,  0.8962],\n",
        "        [-0.0363, -0.7640,  0.0746,  1.2061]], dtype=torch.float32)\n",
        "\n",
        "model = DiscreteModel(gym.make(\"CartPole-v1\"))\n",
        "out = model(input)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "Hkci0ko_Lvvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "AlpDhZwZdVsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_action_from_model(Q, obs):\n",
        "    if not torch.is_tensor(obs):\n",
        "        obs = torch.from_numpy(obs)\n",
        "\n",
        "    pred = Q(obs)\n",
        "    action = torch.argmax(pred, dim=1)\n",
        "    return action\n",
        "\n",
        "# eps percent chance of taking greedy action. otherwise random.\n",
        "def sample_action_eps_greedy(eps, Q_model, obs, env):\n",
        "    if random.random() > eps:\n",
        "        return torch.tensor([[env.action_space.sample()]])\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            return get_action_from_model(Q_model, obs)"
      ],
      "metadata": {
        "id": "2Fv9kvaTAgGK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "Q = DiscreteModel(env)\n",
        "obs = env.reset()\n",
        "\n",
        "input = torch.tensor([[-0.0140, -0.1768,  0.0392,  0.2849],\n",
        "        [-0.0175, -0.3725,  0.0449,  0.5897],\n",
        "        [-0.0250, -0.5682,  0.0567,  0.8962],\n",
        "        [-0.0363, -0.7640,  0.0746,  1.2061]], dtype=torch.float32)\n",
        "\n",
        "input = torch.tensor([[-0.0140, -0.1768,  0.0392,  0.2849]], dtype=torch.float32)\n",
        "\n",
        "a = get_action_from_model(Q, input)\n",
        "\n",
        "# obs = torch.from_numpy(input)\n",
        "print(obs)\n",
        "a = get_action_from_model(Q, input)\n",
        "print(a)\n",
        "\n",
        "a = sample_action_eps_greedy(0.0, Q, input, env)\n",
        "print(a)\n",
        "\n",
        "a = sample_action_eps_greedy(1.0, Q, input, env)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "gxbTZ4JEqCCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n"
      ],
      "metadata": {
        "id": "MLG0_zFeBdbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 500\n",
        "max_rollout = 600\n",
        "batch_size = 512\n",
        "minibatch_size = 128\n",
        "\n",
        "eps = 1.0\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.001\n",
        "decay_rate = 0.08\n",
        "\n",
        "epochs_to_reset_Q = 10\n",
        "\n",
        "gamma = 0.95\n",
        "lr = 0.0005\n"
      ],
      "metadata": {
        "id": "1Z6z9lI7BgvO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "UKwfOBAodYar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_data(env, next_obs, next_done, running_reward, eps, Q, log=False, print_debug=False):\n",
        "    data = [\n",
        "            np.empty((0, env.observation_space.shape[0]), dtype=np.float32), # obs\n",
        "            np.empty((0, 1), dtype=np.float32), # action\n",
        "            np.empty((0, env.observation_space.shape[0]), dtype=np.float32), # next_obs\n",
        "            np.empty((0, 1), dtype=np.float32), # reward\n",
        "            np.empty((0, 1), dtype=bool)  # done\n",
        "            ] \n",
        "    rollout_steps = 0\n",
        "    for step in range(0, batch_size):\n",
        "        obs = next_obs\n",
        "        done = next_done\n",
        "\n",
        "        action = sample_action_eps_greedy(eps, Q, np.reshape(torch.from_numpy(obs), (1,-1)), env)\n",
        "\n",
        "        next_obs, reward, next_done, info = env.step(action[0].item())\n",
        "        running_reward += reward\n",
        "\n",
        "        data[0] = np.append(data[0], np.reshape(obs, (1,-1)), axis=0)\n",
        "        data[1] = np.append(data[1], np.reshape(action, (1,-1)), axis=0)\n",
        "        data[2] = np.append(data[2], np.reshape(next_obs, (1,-1)), axis=0)\n",
        "        data[3] = np.append(data[3], np.reshape(reward, (1,-1)), axis=0)\n",
        "        data[4] = np.append(data[4], np.reshape(done, (1,-1)), axis=0)\n",
        "\n",
        "        if done or rollout_steps > max_rollout:\n",
        "            next_obs = env.reset()\n",
        "            next_done = False\n",
        "\n",
        "            if log:\n",
        "                wandb.log({\"rollout_steps\": rollout_steps})\n",
        "                wandb.log({\"running_reward\": running_reward})\n",
        "            running_reward = 0\n",
        "            rollout_steps = 0\n",
        "\n",
        "        rollout_steps += 1\n",
        "    return data, next_obs, next_done, running_reward\n",
        "\n"
      ],
      "metadata": {
        "id": "wba55KVvtG1B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "Q = DiscreteModel(env)\n",
        "\n",
        "next_obs = env.reset()\n",
        "next_done = False\n",
        "running_reward = 0\n",
        "data, next_obs, next_done, running_reward = collect_data(env, next_obs, next_done, running_reward, 1.0, Q)\n",
        "print(next_obs)\n",
        "print(next_done)\n",
        "print(running_reward)\n",
        "\n",
        "data, next_obs, next_done, running_reward = collect_data(env, next_obs, next_done, running_reward, 0.0, Q)\n",
        "print(next_obs)\n",
        "print(next_done)\n",
        "print(running_reward)\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "0hGIm3FrtoNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(log=False, print_debug=False):\n",
        "    if log:\n",
        "        wandb.init(project=\"DQN\")\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    Q = DiscreteModel(env)\n",
        "    Q_frozen = DiscreteModel(env)\n",
        "\n",
        "    Q.train()\n",
        "\n",
        "    if log: \n",
        "        wandb.watch(Q, log_freq=1)\n",
        "\n",
        "    next_obs = env.reset()\n",
        "    next_done = 0\n",
        "\n",
        "    optimizer = torch.optim.SGD(Q.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    running_reward = 0\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "        if print_debug:\n",
        "            print(\"*****EPOCH {}\".format(epoch))\n",
        "        eps = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*epoch)\n",
        "        if log:\n",
        "            wandb.log({\"epsilon\": eps})\n",
        "            wandb.log({\"epoch\": epoch})\n",
        "        \n",
        "        # Collect batch\n",
        "        data, next_obs, next_done, running_reward = collect_data(\n",
        "            env=env, \n",
        "            next_obs=next_obs,\n",
        "            next_done=next_done,\n",
        "            running_reward=running_reward, \n",
        "            eps=eps,\n",
        "            Q=Q_frozen, \n",
        "            log=log, \n",
        "            print_debug=print_debug\n",
        "        )\n",
        "\n",
        "        if print_debug:\n",
        "            print(\"**TRAIN\")\n",
        "\n",
        "        tensor_obs = torch.from_numpy(data[0])\n",
        "        tensor_action = torch.from_numpy(data[1])\n",
        "        tensor_next_obs = torch.from_numpy(data[2])\n",
        "        tensor_reward = torch.from_numpy(data[3])\n",
        "        tensor_done = torch.from_numpy(data[4])\n",
        "\n",
        "        done_rewards = (tensor_done == True) * tensor_reward\n",
        "        q_next = Q_frozen(tensor_next_obs)\n",
        "        max_q_next = torch.max(q_next, dim=1)\n",
        "        if print_debug:\n",
        "            print(\"max(q_next)\")\n",
        "            print(max_q_next)\n",
        "\n",
        "        not_done_rewards = (tensor_done == False) * (tensor_reward + gamma * max_q_next.values)\n",
        "        td_target = torch.add(done_rewards, not_done_rewards)\n",
        "\n",
        "\n",
        "        if print_debug:\n",
        "            print(\"done rewards\")\n",
        "            print(done_rewards)\n",
        "            print(\"next obs\")\n",
        "            print(next_obs)\n",
        "            print(\"q next\")\n",
        "            print(q_next)\n",
        "            print(\"done\")\n",
        "            print(done)\n",
        "            print(\"not done\")\n",
        "            print(done == False)\n",
        "            print(\"not done rewards\")\n",
        "            print(not_done_rewards)\n",
        "            print(\"td_target\")\n",
        "            print(td_target)\n",
        "\n",
        "        q_with_a = Q(tensor_obs)[:,tensor_action.to(int)]\n",
        "        if print_debug:\n",
        "            print(\"td target\")\n",
        "            print(td_target.to(float))\n",
        "            print(\"q with a\")\n",
        "            print(q_with_a.to(float))\n",
        "\n",
        "        loss = loss_fn(td_target.to(float), q_with_a.to(float))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        if log:\n",
        "            wandb.log({\"loss\": loss})\n",
        "\n",
        "        # Copy weights if the correct epoch\n",
        "        if epoch % epochs_to_reset_Q == 0:\n",
        "            Q_frozen.load_state_dict(Q.state_dict())\n",
        "\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "nXUZ6GDsCkVm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(print_debug=False)"
      ],
      "metadata": {
        "id": "Q2aU5i3_QY3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(log=True)"
      ],
      "metadata": {
        "id": "1hGbr6iK_K2k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}