{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a2c.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uivUVVt7lWxZ",
        "fmA6t3khlbbV",
        "PSbrD6xylfOb"
      ],
      "authorship_tag": "ABX9TyOYKwjuAorovLEQFI1KEmUe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbpacker/deep-rl-class/blob/main/unit7/a2c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My a2c implementation\n",
        "\n",
        "* [view results](https://wandb.ai/jefsnacker/a2c_CartPole-v1?workspace=user-jefsnacker)\n",
        "\n",
        "resources:\n",
        "\n",
        "* [a2c walkthrough](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)\n",
        "* [huggingface class](https://huggingface.co/blog/deep-rl-a2c)\n",
        "* [huggingface a2c](https://github.com/huggingface/deep-rl-class/blob/main/unit7/unit7.ipynb)\n",
        "* [a2c commic](https://cdn.discordapp.com/attachments/997489654565712002/1003348192093540462/unknown.png)\n",
        "* [pytorch implementation](https://github.com/pytorch/examples/blob/main/reinforcement_learning/actor_critic.py)\n",
        "* [single step example](https://medium.com/deeplearningmadeeasy/advantage-actor-critic-a2c-implementation-944e98616b) with [code](https://github.com/hermesdt/reinforcement-learning/blob/master/a2c/cartpole_a2c_online.ipynb)\n"
      ],
      "metadata": {
        "id": "G_hIi2molJKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Everything Ready"
      ],
      "metadata": {
        "id": "Kj6e-_1RlUDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install deps"
      ],
      "metadata": {
        "id": "uivUVVt7lWxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "\n",
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(500, 500))\n",
        "virtual_display.start()\n",
        "\n",
        "!pip install gym\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
        "!pip install git+https://github.com/qlan3/gym-games.git\n",
        "!pip install huggingface_hub\n",
        "!pip install wandb\n",
        "!pip install imageio-ffmpeg\n",
        "\n",
        "!pip install pyyaml==6.0 # avoid key error metadata\n",
        "\n",
        "!pip install pyglet # Virtual Screen"
      ],
      "metadata": {
        "id": "CmIF1CNElIoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "fmA6t3khlbbV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ebJznj-NlFXd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import wandb\n",
        "\n",
        "import gym\n",
        "import gym_pygame\n",
        "\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select training device"
      ],
      "metadata": {
        "id": "PSbrD6xylfOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "NLNhNj6Jlfsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Networks"
      ],
      "metadata": {
        "id": "ML-H33-ilo9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Critic"
      ],
      "metadata": {
        "id": "9LHlYvq2mHd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, num_obs):\n",
        "        \"\"\"\n",
        "        Takes the state as input and outputs Q(s), which is\n",
        "        a vector of Q values for all possible actions\n",
        "        \"\"\"\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        \n",
        "        self.num_obs = num_obs\n",
        "\n",
        "        self.l1 = nn.Linear(num_obs, 128)\n",
        "        self.l2 = nn.Linear(128, 256)\n",
        "        self.l3 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.from_numpy(x).float()\n",
        "        x = self.l1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.l2(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        return self.l3(x)\n",
        "    \n",
        "    ## Used if model output is Q\n",
        "    # def get_all_q(self, state):\n",
        "    #     state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "    #     qs = self.forward(state)\n",
        "    #     return qs\n",
        "\n",
        "    # def get_q(self, state, action):\n",
        "    #     return self.get_all_q(state)[:,action]\n",
        "\n",
        "## Debugging\n",
        "# env = gym.make(\"CartPole-v1\")\n",
        "# c = CriticNetwork(env.observation_space.shape[0], env.action_space.n)\n",
        "# print(c)\n",
        "# s = env.reset()\n",
        "\n",
        "# print(c.get_all_q(s))\n",
        "# print(c.get_q(s, 1))"
      ],
      "metadata": {
        "id": "W1i6uKwAlun1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor"
      ],
      "metadata": {
        "id": "ke_yTmxkmKE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, num_obs, num_act):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        \n",
        "        self.num_obs = num_obs\n",
        "        self.num_act = num_act\n",
        "\n",
        "        self.l1 = nn.Linear(num_obs, 128)\n",
        "        self.l2 = nn.Linear(128, 256)\n",
        "        self.l3 = nn.Linear(256, num_act)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.l2(x)\n",
        "        x = F.relu(x)\n",
        "        action_scores = self.l3(x)\n",
        "        action_probs = F.softmax(action_scores, dim=1)\n",
        "\n",
        "        return action_probs\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        Given a state, take action\n",
        "        \"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        probs = self.forward(state)\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action, m.log_prob(action)"
      ],
      "metadata": {
        "id": "E-b92LwRlr0y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ktuxcFrKmvzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### utils"
      ],
      "metadata": {
        "id": "yQ_bS0eQnInF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_networks(env):\n",
        "    num_obs = env.observation_space.shape[0]\n",
        "    num_act = env.action_space.n\n",
        "\n",
        "    actor = ActorNetwork(num_obs, num_act)\n",
        "    critic = CriticNetwork(num_obs)\n",
        "\n",
        "    return actor, critic"
      ],
      "metadata": {
        "id": "FlODeOjmnfoQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def record_video(env, policy, out_directory, fps=30):\n",
        "    images = []  \n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "    while not done:\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action, _ = policy.act(state)\n",
        "        state, reward, done, info = env.step(action.item()) # We directly put next_state = state for recording logic\n",
        "        img = env.render(mode='rgb_array')\n",
        "        images.append(img)\n",
        "        action.detach()\n",
        "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
        "    wandb.log({\"videos\": wandb.Video(out_directory, fps=fps)})\n",
        "\n",
        "# env_id = \"CartPole-v1\"\n",
        "# env = gym.make(env_id)\n",
        "# policy = PolicyNetwork(num_obs, num_act)\n",
        "# record_video(env, policy, \"/home/out.gif\", fps=30)"
      ],
      "metadata": {
        "id": "CIRZITDCnILn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "8OQe4Ubmmxro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env_id):\n",
        "    if log: \n",
        "        name = \"a2c_\" + env_id\n",
        "        wandb.init(project=name)\n",
        "\n",
        "    env = gym.make(env_id)\n",
        "    reward = 0\n",
        "    done = False\n",
        "\n",
        "    actor, critic = make_networks(env)\n",
        "\n",
        "    if log:\n",
        "        wandb.watch((actor, critic), log_freq=1)\n",
        "\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=a_lr)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=c_lr)\n",
        "\n",
        "\n",
        "    # (next_state)\n",
        "    #      o\n",
        "    next_state = env.reset()\n",
        "\n",
        "    episode_steps = 0\n",
        "    episode_reward = 0\n",
        "    num_episodes = 1\n",
        "\n",
        "    for step in range(1, n_steps):\n",
        "        #      (state)\n",
        "        #  (-->)  o\n",
        "        state = next_state\n",
        "\n",
        "        #      (state)  r,a  (next_state)\n",
        "        #  (-->)  o ------------> o\n",
        "        action, log_prob = actor.act(state)\n",
        "        next_state, reward, done, info = env.step(action.item())\n",
        "        if done:\n",
        "            advantage = reward - critic(state)\n",
        "        else:\n",
        "            advantage = reward + gamma*critic(next_state) - critic(state)\n",
        "\n",
        "        episode_steps += 1\n",
        "        episode_reward += reward\n",
        "\n",
        "        ## update critic\n",
        "        critic_loss = advantage.pow(2).mean()\n",
        "        critic_loss.backward()\n",
        "\n",
        "        critic_optimizer.step()\n",
        "        critic_optimizer.zero_grad()\n",
        "\n",
        "        ## update actor\n",
        "        # detach advantage to update the 2nd network\n",
        "        actor_loss = -log_prob * advantage.detach()\n",
        "        actor_loss.backward()\n",
        "\n",
        "        actor_optimizer.step()\n",
        "        actor_optimizer.zero_grad()\n",
        "\n",
        "        ## If done next step them reset env\n",
        "        if done or episode_steps > max_episode_steps:\n",
        "\n",
        "            if log:\n",
        "                wandb.log({\"episode_steps\": episode_steps,\n",
        "                           \"episode_reward\": episode_reward,\n",
        "                           \"num_episodes\": num_episodes})\n",
        "\n",
        "            # (next_state, next_done)\n",
        "            #           o\n",
        "            next_state = env.reset()\n",
        "            next_done = False\n",
        "            episode_steps = 0\n",
        "            episode_reward = 0\n",
        "            num_episodes += 1\n",
        "\n",
        "\n",
        "        if log and step % log_rate == 0:\n",
        "            wandb.log({\"step\": step, \n",
        "                       \"actor_loss\": actor_loss,\n",
        "                       \"critic_loss\": critic_loss,\n",
        "                       \"advantage\": advantage})\n",
        "            if record_vids and num_episodes % num_episodes_to_vid == 0:\n",
        "                record_video(env, actor, \"/out.mp4\")"
      ],
      "metadata": {
        "id": "x3e9FyXpmxRW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### episodic implementation\n",
        "[example](https://github.com/hermesdt/reinforcement-learning/blob/master/a2c/cartpole_a2c_episodic.ipynb)"
      ],
      "metadata": {
        "id": "y39bWjOGMppk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_episodic(env_id):\n",
        "    if log: \n",
        "        name = \"a2c_\" + env_id\n",
        "        wandb.init(project=name)\n",
        "\n",
        "    env = gym.make(env_id)\n",
        "    reward = 0\n",
        "    done = False\n",
        "\n",
        "    actor, critic = make_networks(env)\n",
        "\n",
        "    if log:\n",
        "        wandb.watch((actor, critic), log_freq=1)\n",
        "\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=a_lr)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=c_lr)\n",
        "\n",
        "    num_episodes = 1\n",
        "    episode_steps = 1\n",
        "    episode_reward = 0\n",
        "    next_state = env.reset()\n",
        "\n",
        "    # each episode collects 1 episode or N steps and trains\n",
        "    for epoch in range(n_epochs):\n",
        "        # (next_state)\n",
        "        #      o\n",
        "        advantages = []\n",
        "        log_probs = []\n",
        "\n",
        "        for step in range(0, steps_per_epoch):\n",
        "            #      (state)\n",
        "            #  (-->)  o\n",
        "            state = next_state\n",
        "\n",
        "            #      (state)  r,a  (next_state)\n",
        "            #  (-->)  o ------------> o\n",
        "            action, log_prob = actor.act(state)\n",
        "            next_state, reward, done, info = env.step(action.detach().item())\n",
        "            if done:\n",
        "                advantage = reward - critic(state)\n",
        "            else:\n",
        "                advantage = reward + gamma*critic(next_state) - critic(state)\n",
        "\n",
        "            advantages.append(advantage)\n",
        "            log_probs.append(log_prob)\n",
        "\n",
        "            episode_reward += reward\n",
        "            episode_steps += 1\n",
        "\n",
        "            # print(\"[{}] a: {} r: {} d: {}\".format(step, advantage, reward, done))\n",
        "\n",
        "            if done or episode_steps > max_episode_steps:\n",
        "                if log:\n",
        "                    wandb.log({\n",
        "                        \"episode_steps\": episode_steps,\n",
        "                        \"episode_reward\": episode_reward,\n",
        "                        \"num_epochs\": epoch,\n",
        "                        \"num_episodes\": num_episodes,\n",
        "                    })\n",
        "\n",
        "                num_episodes += 1\n",
        "                episode_reward = 0\n",
        "                episode_steps = 1\n",
        "                next_state = env.reset()\n",
        "\n",
        "        ## update critic\n",
        "        advantage = torch.stack(advantages)\n",
        "\n",
        "        critic_loss = advantage.pow(2).mean()\n",
        "        critic_loss.backward()\n",
        "\n",
        "        critic_optimizer.step()\n",
        "        critic_optimizer.zero_grad()\n",
        "\n",
        "        ## update actor\n",
        "        # detach advantage to update the 2nd network\n",
        "        actor_loss = (-torch.stack(log_probs) * advantage.detach()).mean()\n",
        "        actor_loss.backward()\n",
        "\n",
        "        actor_optimizer.step()\n",
        "        actor_optimizer.zero_grad()\n",
        "\n",
        "        ## If done next step them reset env\n",
        "        if log:\n",
        "            wandb.log({\n",
        "                \"actor_loss\": actor_loss,\n",
        "                \"critic_loss\": critic_loss,\n",
        "                \"epoch\": epoch,\n",
        "            })\n",
        "            if record_vids and epoch % num_episodes_to_vid == 0:\n",
        "                    record_video(env, actor, \"/content/out.mp4\")"
      ],
      "metadata": {
        "id": "ujkB8schA40v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare to stable baselines 3 a2c implementation"
      ],
      "metadata": {
        "id": "egyapJIA0cIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### My code\n"
      ],
      "metadata": {
        "id": "-SWwTch35raI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a_lr = 1e-3\n",
        "c_lr = 1e-3\n",
        "gamma = 0.99\n",
        "n_epochs = 5000\n",
        "steps_per_epoch = 128\n",
        "max_episode_steps = 500\n",
        "\n",
        "log = True\n",
        "num_episodes_to_vid = 100\n",
        "record_vids = True\n",
        "\n",
        "env_id = \"CartPole-v1\"\n",
        "# env_id = \"Pixelcopter-PLE-v0\"\n",
        "\n",
        "train_episodic(env_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8mClsHJgxHE",
        "outputId": "236e45ca-6f12-4df8-dfda-c2d4b43bd434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 1000000\n",
        "a_lr = 1e-3\n",
        "c_lr = 1e-3\n",
        "gamma = 0.99\n",
        "max_episode_steps = 500\n",
        "\n",
        "log = True\n",
        "log_rate = 1\n",
        "num_episodes_to_vid = 500\n",
        "record_vids = True\n",
        "\n",
        "# env_id = \"CartPole-v1\"\n",
        "env_id = \"Pixelcopter-PLE-v0\"\n",
        "\n",
        "train(env_id)"
      ],
      "metadata": {
        "id": "pweeYz4C0a_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stable baselines 3 implementation"
      ],
      "metadata": {
        "id": "MKY9X8ru5uaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import A2C\n",
        "\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
        "\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "\n",
        "env_id = \"CartPole-v1\"\n",
        "# env_id = \"Pixelcopter-PLE-v0\"\n",
        "policy = 'MlpPolicy'\n",
        "\n",
        "config = {\n",
        "    \"env_id\": env_id,\n",
        "    \"policy\": policy,\n",
        "}\n",
        "record_video_every_n_steps = 50000\n",
        "total_timesteps = 400000\n",
        "\n",
        "\n",
        "## Set up logging\n",
        "name = \"a2c_\" + env_id\n",
        "run = wandb.init(project=name, \n",
        "                 config=config,\n",
        "                 sync_tensorboard=True, # auto-upload sb3's tensorboard metrics\n",
        "                 monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
        "                 save_code=True)\n",
        "\n",
        "## Make the environment\n",
        "def make_env():\n",
        "    env = gym.make(config[\"env_id\"])\n",
        "    env = Monitor(env)  # record stats such as returns\n",
        "    return env\n",
        "\n",
        "env = DummyVecEnv([make_env] * 1) # 1 simulation\n",
        "env = VecVideoRecorder(\n",
        "    env, \n",
        "    f\"videos/{run.id}\", \n",
        "    record_video_trigger=lambda x: x % record_video_every_n_steps == 0, \n",
        "    video_length=200\n",
        ")\n",
        "\n",
        "# Custom actor (pi) and value function (vf) networks\n",
        "# of two layers of size 32 each with Relu activation function\n",
        "policy_kwargs = dict(activation_fn=torch.nn.ReLU,\n",
        "                     net_arch=[dict(pi=[128, 256], vf=[128, 256])])\n",
        "# Create the agent\n",
        "model = A2C(\"MlpPolicy\", env_id, policy_kwargs=policy_kwargs, verbose=1)\n",
        "\n",
        "\n",
        "## Make the model\n",
        "model = A2C(\n",
        "    policy = 'MlpPolicy',\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    env = env,\n",
        "    n_steps = 50000,\n",
        "    # learning_rate=linear_schedule(init_learning_rate),\n",
        "    # batch_size = batch_size,\n",
        "    tensorboard_log=f\"runs/{run.id}\"\n",
        ") \n",
        "\n",
        "## Train!\n",
        "model.learn(\n",
        "    total_timesteps=total_timesteps,\n",
        "    callback=WandbCallback(\n",
        "        verbose=2,\n",
        "        model_save_path=f\"models/{run.id}\"\n",
        "    )\n",
        ")\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "zhcU4ju_009W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}